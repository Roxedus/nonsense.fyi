[{"content":"Over at Linuxserver we use Ansible for it\u0026rsquo;s convenient integration with Jinja, a templating engine for Python, along with its powerful framework to execute shell commands. With this we are able to automate quite a lot of work, like generating a uniform and recognizable README, or injecting files into the container(don\u0026rsquo;t worry, the files are committed and pushed to GitHub before the image is built) for some logic. In order for all this to work, most of the metadata is filled into two YAML files in each repository, jenkins-vars.yml and readme-vars.yml which are then presented to Ansible as variables used for consumption in the templates.\nWhy? Having all the metadata stored in a couple of files that can be pragmatically read makes it relatively easy to expand what we are able to output, and ensuring changes propagates trough all outputs.\nAside from the Dockerfiles, the mentioned var files, and most of the root/ folder, the rest of the repository are actually templated. The benefit of this approach is huge, as we often only need to update a file once, and the changes are done in the repositories as they receive updates, like when we enabled and started promoting GHCR as the default registry for all our images, with a single pull request.\nIf you are used to the(now paid) feature of DockerHub, rebuilding images when the repository updated on GitHub, one would be accustomed to the idea of the readme on DockerHub to always reflect the readme on GitHub, however that is not the case, regardless if the repositories are linked. It is traditionally a thing you have to update by hand, however with some creative thinking you can update this with code, which we do. This means that the readme on GitHub and DockerHub is always up-to-date and identical(as long as it\u0026rsquo;s not to long for DockerHub). There\u0026rsquo;s also a derivative from the readme published to the documentation site for each image, going more in-depth for sections of the readme. To up the inception scale, we also template the CI/CD pipeline, from the small stuff like the greetings bot to the whole Jenkinsfile used to build, test and push the images.\nRemembering the once-in-a-while tasks As I touched on in the blog post announcing automated Unraid templates, creating templates for Unraid was a manual task, often depending on someone in the Linuxserver team using Unraid to actually creating one, this could mean it had the potential to take days or even weeks to push a template. As there is a decent amount of tasks tied to launching a image it might even be forgotten, so automating this step would be better for everyone involved.\nHow? As outlined earlier the important building-blocks are present, a templating engine and repository-level metadata, despite this I had to create some new blocks.\nThis adventure started with getting reacquainted to XML, as that\u0026rsquo;s how the Unraid templates are stored, remembering the specification will surely help with some future headaches.\nGetting started I start by making some helpful notes to any potential contributor wanting to help us maintaining the template, pointing them to the correct file for changing the output. As this is done in the template, the full address for the readme-vars.yml file will point to the actual repository.\n40 41 42 \u0026lt;?xml version=\u0026#34;1.0\u0026#34;?\u0026gt; \u0026lt;!-- DO NOT CHANGE THIS FILE MANUALLY, IT IS AUTOMATICALLY GENERATED --\u0026gt; \u0026lt;!-- GENERATED FROM {{ project_github_asset }}/readme-vars.yml --\u0026gt; Simple string substitution and conditionals Now it\u0026rsquo;s time to create the stuff that actually matters for Unraid, this is stored under the Container tag in the XML file.\n43 44 45 46 47 48 49 50 51 52 53 54 55 56 \u0026lt;Container version=\u0026#34;2\u0026#34;\u0026gt; \u0026lt;Name\u0026gt;{{ param_container_name | lower }}\u0026lt;/Name\u0026gt; \u0026lt;Repository\u0026gt;lscr.io/{{ lsio_project_name_short }}/{{ project_name }}\u0026lt;/Repository\u0026gt; \u0026lt;Registry\u0026gt;https://github.com/orgs/{{ lsio_project_name_short }}/packages/container/package/{{ project_name }}\u0026lt;/Registry\u0026gt; \u0026lt;Network\u0026gt;{{ param_net if param_usage_include_net is sameas true else \u0026#39;bridge\u0026#39; }}\u0026lt;/Network\u0026gt; \u0026lt;Privileged\u0026gt;{{ \u0026#34;true\u0026#34; if privileged is sameas true else \u0026#34;false\u0026#34; }}\u0026lt;/Privileged\u0026gt; \u0026lt;Support\u0026gt;{{ project_github_repo_url }}/issues/new/choose\u0026lt;/Support\u0026gt; \u0026lt;Shell\u0026gt;bash\u0026lt;/Shell\u0026gt; \u0026lt;ReadMe\u0026gt;{{ project_github_repo_url }}{{ \u0026#34;#readme\u0026#34; }}\u0026lt;/ReadMe\u0026gt; \u0026lt;Project\u0026gt;{{ project_url }}\u0026lt;/Project\u0026gt; \u0026lt;Overview\u0026gt;{{ ca(project_blurb) | trim }}\u0026lt;/Overview\u0026gt; \u0026lt;GitHub\u0026gt;{{ project_github_repo_url }}{{ \u0026#34;#application-setup\u0026#34; if app_setup_block_enabled is defined and app_setup_block_enabled }}\u0026lt;/GitHub\u0026gt; \u0026lt;TemplateURL\u0026gt;{{ \u0026#34;false\u0026#34; if unraid_template_sync is sameas false else \u0026#34;https://raw.githubusercontent.com/linuxserver/templates/main/unraid/\u0026#34; + project_name | lower + \u0026#34;.xml\u0026#34; }}\u0026lt;/TemplateURL\u0026gt; \u0026lt;Icon\u0026gt;https://raw.githubusercontent.com/linuxserver/docker-templates/master/linuxserver.io/img/linuxserver-ls-logo.png\u0026lt;/Icon\u0026gt; There is a few things happening here, mostly normal substituting of variables, there is also some transforming done, as \u0026quot;\u0026quot; is not a valid value and literal booleans needed to be it\u0026rsquo;s string counterpart, along with some logic to conditionally append a link. The rest of the template consists mostly of these types of substitutions and transformations. We will get to ca() later\nGoing in loops Unraid templates support multiple branches. When installing from a template with multiple branches defined using Community Applications, you will get prompted with a selection box with all the branches listed in the template. To populate these fields, I iterate from the same variable that lists the branches on the readme, however I have recently added some filtering here to avoid listing deprecated branches.\n59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 {# Set the Branches, if any Config items is overwritten. TODO: handle config items #} {% if development_versions is defined and development_versions == \u0026#34;true\u0026#34; %} {% for item in development_versions_items if not \u0026#34;deprecate\u0026#34; in item.desc.lower() %} \u0026lt;Branch\u0026gt; \u0026lt;Tag\u0026gt;{{ ca(item.tag) }}\u0026lt;/Tag\u0026gt; \u0026lt;TagDescription\u0026gt;{{ ca(item.desc) }}\u0026lt;/TagDescription\u0026gt; {% if item.tag != \u0026#34;latest\u0026#34; %} \u0026lt;ReadMe\u0026gt;{{ project_github_repo_url }}{{ \u0026#34;/tree/\u0026#34; + item.tag + \u0026#34;#readme\u0026#34; }}\u0026lt;/ReadMe\u0026gt; \u0026lt;GitHub\u0026gt;{{ project_github_repo_url }}{{ (\u0026#34;/tree/\u0026#34; + item.tag + \u0026#34;#application-setup\u0026#34;) if app_setup_block_enabled is defined and app_setup_block_enabled }}\u0026lt;/GitHub\u0026gt; {% endif %} {% if item.extra is defined %} {#- Allow for branch-specific stuff #} {{ ca(item.extra) | indent(8) | trim }} {% endif %} \u0026lt;/Branch\u0026gt; {% endfor %} {% endif %} {# Set the Branches, if any #} This snippet is just a simple loop going over the development_versions_items list of arrays if development_versions exists. The following readme-vars.yml produced the above screenshot:\n1 2 3 4 5 6 7 # development version development_versions: true development_versions_items: - { tag: \u0026#34;latest\u0026#34;, desc: \u0026#34;Stable Radarr releases\u0026#34; } - { tag: \u0026#34;develop\u0026#34;, desc: \u0026#34;Radarr releases from their develop branch\u0026#34; } - { tag: \u0026#34;nightly\u0026#34;, desc: \u0026#34;Radarr releases from their nightly branch\u0026#34; } - { tag: \u0026#34;nightly-alpine\u0026#34;, desc: \u0026#34;Radarr releases from their nightly branch using our Alpine baseimage\u0026#34; } I took the opportunity to add a key called extra for the dictionary, as CA has the ability to have separate config variables per branch. Unfortunately this is implemented in a way which makes it hard to use, the presence of any branch-specific items disregards all other config tags specified in the Container tag. Meaning that a dictionary like { tag: \u0026quot;nightly\u0026quot;, desc: \u0026quot;Radarr releases from their nightly branch\u0026quot;, extra: { nightly_var: \u0026quot;Do monkeydance\u0026quot;} } would render all other configuration items(such as environment variables, bind mounts and port mappings) void, if this branch was chosen. This is something I might have to account for at some time, by essentially generating the same values once per branch.\nBaby\u0026rsquo;s first macro The next part I wanted to tackle, was building the link for the WebUI, here I had to be creative, while the information needed was present, it is not easily accessible as it is stored in the format Groovy wants variables to be presented in a Jenkinsfile. The input would look like this for the SWAG image:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 repo_vars: - EXT_PIP = \u0026#39;certbot\u0026#39; - BUILD_VERSION_ARG = \u0026#39;CERTBOT_VERSION\u0026#39; - LS_USER = \u0026#39;linuxserver\u0026#39; - LS_REPO = \u0026#39;docker-swag\u0026#39; - CONTAINER_NAME = \u0026#39;swag\u0026#39; - DOCKERHUB_IMAGE = \u0026#39;linuxserver/swag\u0026#39; - DEV_DOCKERHUB_IMAGE = \u0026#39;lsiodev/swag\u0026#39; - PR_DOCKERHUB_IMAGE = \u0026#39;lspipepr/swag\u0026#39; - DIST_IMAGE = \u0026#39;alpine\u0026#39; - MULTIARCH=\u0026#39;true\u0026#39; - CI=\u0026#39;true\u0026#39; - CI_WEB=\u0026#39;false\u0026#39; - CI_PORT=\u0026#39;80\u0026#39; - CI_SSL=\u0026#39;false\u0026#39; - CI_DELAY=\u0026#39;30\u0026#39; - CI_DOCKERENV=\u0026#39;TEST_RUN=1\u0026#39; - CI_AUTH=\u0026#39;\u0026#39; - CI_WEBPATH=\u0026#39;\u0026#39; Getting the value of CI_PORT is not as easy as it should be, like using a getter on repo_vars does not work. Fortunately we can use some clever replacements and splits of each item under repo_vars to build a new and usable variable.\n1 2 3 4 5 6 {#- Create a real object from repo_vars -#} {%- set better_vars={} -%} {%- for i in repo_vars -%} {%- set i=(i | replace(\u0026#39; = \u0026#39;, \u0026#39;=\u0026#39;, 1) | replace(\u0026#39;=\u0026#39;, \u0026#39;¯\\_(ツ)_/¯\u0026#39;, 1) | replace(\u0026#34;\u0026#39;\u0026#34;, \u0026#34;\u0026#34;) | replace(\u0026#39;\u0026#34;\u0026#39;, \u0026#34;\u0026#34;)).split(\u0026#39;¯\\_(ツ)_/¯\u0026#39;) -%} {%- set x=(better_vars.__setitem__(i[0], i[1])) -%} {%- endfor -%} The new variable this creates is called better_vars, which is a dictionary-type. I and X is used as throwaway variables, as Jinja does not really have a good way to run straight up code(and with good reason, I imagine). Since repo_vars is a array-type, it serves as a iterator, and saves me from even more bodging. First order of business is to make the list uniform across both ways of placing the equals-sign, after this they don´t have any padding with spaces, I can start replacing and splitting the rest of the line until it has some resemblance of a typical python-like key-value pair.\nIn the first iteration of this code, there was no shrug, but a carefully chosen example above highlights how the split would work against us, the macro would fail when it arrived at CI_DOCKERENV, as the value of that key, is a key-value pair.\nWe are now working in Python land, so we can remove both double and single quotes, this can come back and bite us later, but as it stands right now this is not an issue. Currently CI_DOCKERENV='TEST_RUN=1' would be CI_DOCKERENV¯\\_(ツ)_/¯TEST_RUN=1, this is not useful yet, but we just need to convert this string to a key-value pair, easily done by using ¯\\_(ツ)_/¯ as the deliminator. Once we have this key-value pair we can use the __setitem__ function of the built in python dictionary-type.\nAfter all that there is now a variable that\u0026rsquo;s easier to work with, simply by using a getter.\n82 83 84 85 {# Set the WebUI link based on the link the CI runs against #} {% if better_vars.get(\u0026#34;CI_WEB\u0026#34;) and better_vars.get(\u0026#34;CI\u0026#34;) == \u0026#34;true\u0026#34; %} \u0026lt;WebUI\u0026gt;{{ \u0026#34;https\u0026#34; if better_vars.get(\u0026#34;CI_SSL\u0026#34;) == \u0026#34;true\u0026#34; else \u0026#34;http\u0026#34; }}://[IP]:[PORT:{{ better_vars.get(\u0026#34;CI_PORT\u0026#34;) }}]\u0026lt;/WebUI\u0026gt; {% endif %} This value is not supposed to hold a real URL, just the parts necessary for Unraid to build one. To do this it needs to know what container port the application is running on, we do this by using the syntax [PORT:80]. Now, if a user maps the container port of 80 to say host port 180, the Unraid webui button would now point to the ip of Unraid with port 180.\nMacros save the day When we told Squid(the guy running Community Applications) to switch us over to the new repo, we actually got blacklisted in CA because I forgot how more-than and the less-than sign got treated both by xml and CA(Community Applications) specifically. In CA they are blacklisted characters, simply having them in the user-facing parts of the template gets the whole template repository blacklisted. This prompted a new macro, one to filter out the illegal characters.\n29 30 31 {%- macro ca(str) -%} {{ str | replace(\u0026#34;\u0026lt;\u0026#34;, \u0026#34;\u0026#34;) | replace(\u0026#34;\u0026gt;\u0026#34;, \u0026#34;\u0026#34;) | replace(\u0026#34;[\u0026#34;, \u0026#34;\u0026#34;) | replace(\u0026#34;]\u0026#34;, \u0026#34;\u0026#34;) | replace(\u0026#34;\u0026amp;\u0026#34;, \u0026#34;and\u0026#34;) | escape }} {%- endmacro -%} This macro simply replaces \u0026lt;, \u0026gt;, [ and ] with nothing, while turning \u0026amp; to a word. For extra safety I put the escape filter at the end. At the time of writing there is no supported syntax in CA to make a hyperlink from a word. All \u0026ldquo;free text\u0026rdquo; input in the template goes trough this filter to prevent another blacklisting.\nGetting warm Now that I have gotten the taste, and gist of using macros, I made another couple of them to keep myself DRY(Don´t Repeat Yourself)\n32 33 34 35 36 37 38 {%- macro readme_date(str) -%} {%- set _date = (str | replace(\u0026#34;:\u0026#34;,\u0026#34;\u0026#34;)).split(\u0026#34;.\u0026#34;) -%} {{ \u0026#34;20\u0026#34; + _date[2] + \u0026#34;-\u0026#34; + _date[1] + \u0026#34;-\u0026#34; + _date[0] }} {%- endmacro -%} {%- macro mask(str) -%} {{ \u0026#34;true\u0026#34; if [\u0026#34;token\u0026#34;, \u0026#34;pass\u0026#34; ,\u0026#34;key\u0026#34;]|select(\u0026#34;in\u0026#34;, str|lower) else \u0026#34;false\u0026#34; }} {%- endmacro -%} Since the schema made for CA supports showing a changelog, we might as well use it, the metadata needed is already present in readme-vars.yml so no real work to get the data is needed. As this is the internet, and people come from different places, the dateformat we use is of course incompatible with the one CA accept, so I made a macro to convert mm.dd.yy to yyyy.mm.dd. Next up is a macro that gets called when creating environment variables, to determine if the variable should be masked.\nAlong with a entry to list potential requirements, the changelog macro is used like this:\n89 90 91 92 93 94 95 96 97 98 99 100 101 102 {% if unraid_requirement is defined and unraid_requirement != \u0026#34;\u0026#34; %} \u0026lt;Requires\u0026gt;{{ unraid_requirement }}\u0026lt;/Requires\u0026gt; {% endif %} {# Create changelog #} {% if changelogs is defined and changelogs %} \u0026lt;Date\u0026gt;{{ readme_date(changelogs |map(attribute=\u0026#39;date\u0026#39;) | first) }}\u0026lt;/Date\u0026gt; \u0026lt;Changes\u0026gt; {% for item in changelogs %} ### {{ readme_date( item.date ) }} - {{ ca(item.desc) }} {% endfor %} \u0026lt;/Changes\u0026gt; {% endif %} The long boi Another thing you might want to do with your container is passing along some less common parameters, like memory or cpu limits. This is something I had to tackle with \u0026ldquo;\u0026ldquo;code\u0026rdquo;\u0026rdquo;. As the metadata for this is more literal to the real compose way of writing it, implementing support for security options is also coming.\n8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 {#- Create ExtraParam for REQUIRED stuff-#} {%- set ExtraParam=[] -%} {%- set x=ExtraParam.append(\u0026#34;--hostname=\u0026#34; + param_hostname) if param_usage_include_hostname is sameas true -%} {%- set x=ExtraParam.append(\u0026#34;--mac-address=\u0026#34; + param_mac_address) if param_usage_include_mac_address is sameas true -%} {%- if cap_add_param is defined -%} {%- for item in cap_add_param_vars -%} {%- set x=ExtraParam.append(\u0026#34;--cap-add=\u0026#34; + item.cap_add_var) -%} {%- endfor -%} {#- custom_params -#} {%- if custom_params is defined -%} {%- for item in custom_params -%} {%- if item.array is not defined -%} {%- set x=ExtraParam.append(\u0026#34;--\u0026#34; + item.name+ \u0026#34;=\u0026#34; + item.value) -%} {%- else -%} {%- for item2 in item.value -%} {%- set x=ExtraParam.append(\u0026#34;--\u0026#34; + item.name+ \u0026#34;=\u0026#34; + item2) -%} {%- endfor -%} {%- endif -%} {%- endfor -%} {%- endif -%} {%- endif -%} This logic defines a variable called ExtraParam, then massages different entries from the metadata, to a array of strings, where each item in the array is a valid docker run argument.\nThe normal stuff This article is not written in a chronological order based on the development cycle, rather following the structure in the end product. You can see this by the lack of macros in the rest of the template, if I ever have to do major revisions of this template, turning this into macros would be the first thing to do.\nPorts A good chunk of the applications we bundle, uses multiple ports for different purposes, this is why we have sections in our metadata for optional ports. Thankfully Unraid has the ability to display if a port is optional or not. The schema also exposes the protocol part of a port mapping, a value we also have support for in our metadata. Now we you can see the CA macro in action, it is used to clean characters from our metadata.\nThere is a lot of logic present to build the description and name of these ports. It will automatically name the first port as \u0026ldquo;WebUI\u0026rdquo;, or it will fall back to the naming Unraid would use. For the description it will use the one defined in the metadata, or fall back to the value Unraid would have used. Mostly the same logic is used in the optional ports.\n103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 {# Set required ports, gets the name from the name atribute if present, or \u0026#34;WebUI\u0026#34; if it is the first port #} {% if param_usage_include_ports | default(false) %} {% for item in param_ports %} {% set port, proto=item.internal_port.split(\u0026#39;/\u0026#39;) if \u0026#34;/\u0026#34; in item.internal_port else [item.internal_port, false] %} {#- Logic to get the protocol #} \u0026lt;Config Name=\u0026#34;{{ ca(item.name) if item.name is defined else \u0026#34;WebUI\u0026#34; if loop.first else \u0026#34;Port: \u0026#34; + port }}\u0026#34; Target=\u0026#34;{{ port }}\u0026#34; Default=\u0026#34;{{ ca(item.external_port) }}\u0026#34; Mode=\u0026#34;{{ proto if proto else \u0026#34;tcp\u0026#34; }}\u0026#34; Description=\u0026#34;{{ ca(item.port_desc) if item.port_desc is defined else \u0026#34;Container Port: \u0026#34; + port }}\u0026#34; Type=\u0026#34;Port\u0026#34; Display=\u0026#34;always\u0026#34; Required=\u0026#34;true\u0026#34; Mask=\u0026#34;false\u0026#34;/\u0026gt; {% endfor %} {% endif %} {#- Set required ports, gets the name from the name atribute if present, or \u0026#34;WebUI\u0026#34; if it is the first port #} {#- Set optional ports #} {% if opt_param_usage_include_ports | default(false) %} {% for item in opt_param_ports %} {% set port, proto=item.internal_port.split(\u0026#39;/\u0026#39;) if \u0026#34;/\u0026#34; in item.internal_port else [item.internal_port, false] %} {#- Logic to get the protocol #} \u0026lt;Config Name=\u0026#34;{{ ca(item.name) if item.name is defined else \u0026#34;Port: \u0026#34; + port }}\u0026#34; Target=\u0026#34;{{ port }}\u0026#34; Default=\u0026#34;{{ ca(item.external_port) }}\u0026#34; Mode=\u0026#34;{{ proto if proto else \u0026#34;tcp\u0026#34; }}\u0026#34; Description=\u0026#34;{{ ca(item.port_desc) if item.port_desc is defined else \u0026#34;Container Port: \u0026#34; + port }}\u0026#34; Type=\u0026#34;Port\u0026#34; Display=\u0026#34;always\u0026#34; Required=\u0026#34;false\u0026#34; Mask=\u0026#34;false\u0026#34;/\u0026gt; {% endfor %} {% endif %} {#- Set optional ports #} Volumes The logic used for volumes is pretty much a copy-paste from the ports-logic, but instead of looking \u0026ldquo;WebUI\u0026rdquo;, it is trying to find a volume to call \u0026ldquo;Appdata\u0026rdquo;. There is also a piece of extra logic to see if a bind-volume is marked as Read Only.\n126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 {#- Set required volumes, gets the name from the name atribute if present, or \u0026#34;Appdata\u0026#34; if it is the /config location #} {% if param_usage_include_vols | default(false) %} {% for item in param_volumes %} {% set path, mode=item.vol_path.split(\u0026#39;:\u0026#39;) if \u0026#34;:\u0026#34; in item.vol_path else [item.vol_path, false] %} {#- Logic to get the mode #} \u0026lt;Config Name=\u0026#34;{{ ca(item.name) if item.name is defined else \u0026#34;Appdata\u0026#34; if path == \u0026#34;/config\u0026#34; else \u0026#34;Path: \u0026#34; + path }}\u0026#34; Target=\u0026#34;{{ ca(path) }}\u0026#34; Default=\u0026#34;{{ ca(item.vol_host_path) if item.default is defined and item.default is sameas true }}\u0026#34; Mode=\u0026#34;{{ mode if mode else \u0026#34;rw\u0026#34; }}\u0026#34; Description=\u0026#34;{{ ca(item.desc) if item.desc is defined else \u0026#34;Path: \u0026#34; + path }}\u0026#34; Type=\u0026#34;Path\u0026#34; Display=\u0026#34;{{ \u0026#34;advanced\u0026#34; if path == \u0026#34;/config\u0026#34; else \u0026#34;always\u0026#34; }}\u0026#34; Required=\u0026#34;true\u0026#34; Mask=\u0026#34;false\u0026#34;/\u0026gt; {% endfor %} {% endif %} {#- Set required volumes, gets the name from the name atribute if present, or \u0026#34;Appdata\u0026#34; if it is the /config location #} {#- Set optional volumes #} {% if opt_param_usage_include_vols | default(false) %} {% for item in opt_param_volumes %} {% set path, mode=item.vol_path.split(\u0026#39;:\u0026#39;) if \u0026#34;:\u0026#34; in item.vol_path else [item.vol_path, false] %} {#- Logic to get the mode #} \u0026lt;Config Name=\u0026#34;{{ ca(item.name) if item.name is defined else \u0026#34;Appdata\u0026#34; if path == \u0026#34;/config\u0026#34; else \u0026#34;Path: \u0026#34; + path }}\u0026#34; Target=\u0026#34;{{ ca(path) }}\u0026#34; Default=\u0026#34;{{ ca(item.vol_host_path) if item.default is defined and item.default is sameas true }}\u0026#34; Mode=\u0026#34;{{ mode if mode else \u0026#34;rw\u0026#34; }}\u0026#34; Description=\u0026#34;{{ ca(item.desc) if item.desc is defined else \u0026#34;Path: \u0026#34; + path }}\u0026#34; Type=\u0026#34;Path\u0026#34; Display=\u0026#34;always\u0026#34; Required=\u0026#34;false\u0026#34; Mask=\u0026#34;false\u0026#34;/\u0026gt; {% endfor %} {% endif %} {#- Set optional volumes #} Variables The base of the logic for variables is also based on the ports-logic, but it does filter away some variables we hardcode, or variables that Unraid automatically manages.\nThe id´s for puid and guid in Unraid, is following a agreed upon id from the early days, the 99 user is ´nobody´.\n135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 {% set skip_envs=[\u0026#34;puid\u0026#34;, \u0026#34;pgid\u0026#34;, \u0026#34;tz\u0026#34;, \u0026#34;umask\u0026#34;] %} {#- Drop envs that are either hardcoded, or automaticcly added by unraid #} {#- Set required variables, gets the name from the name atribute #} {% if param_usage_include_env | default(false) %} {% for item in param_env_vars if not item.env_var | lower is in skip_envs %} \u0026lt;Config Name=\u0026#34;{{ ca(item.name) if item.name is defined else item.env_var }}\u0026#34; Target=\u0026#34;{{ item.env_var }}\u0026#34; Default=\u0026#34;{{ item.env_options | join(\u0026#39;|\u0026#39;) if item.env_options is defined else ca(item.env_value) }}\u0026#34; Description=\u0026#34;{{ ca(item.desc) if item.desc is defined else \u0026#34;Variable: \u0026#34; + path }}\u0026#34; Type=\u0026#34;Variable\u0026#34; Display=\u0026#34;always\u0026#34; Required=\u0026#34;true\u0026#34; Mask=\u0026#34;{{ mask(item.env_var) }}\u0026#34;/\u0026gt; {% endfor %} {% endif %} {#- Set required variables, gets the name from the name atribute #} {#- Set optional variables #} {% if opt_param_usage_include_env | default(false) %} {% for item in opt_param_env_vars if not item.env_var | lower is in skip_envs %} \u0026lt;Config Name=\u0026#34;{{ ca(item.name) if item.name is defined else item.env_var }}\u0026#34; Target=\u0026#34;{{ item.env_var }}\u0026#34; Default=\u0026#34;{{ ca(item.env_value) }}\u0026#34; Description=\u0026#34;{{ ca(item.desc) if item.desc is defined else \u0026#34;Variable: \u0026#34; + path }}\u0026#34; Type=\u0026#34;Variable\u0026#34; Display=\u0026#34;always\u0026#34; Required=\u0026#34;false\u0026#34; Mask=\u0026#34;{{ mask(item.env_var) }}\u0026#34;/\u0026gt; {% endfor %} {% endif %} {#- Set optional variables #} \u0026lt;Config Name=\u0026#34;PUID\u0026#34; Target=\u0026#34;PUID\u0026#34; Default=\u0026#34;99\u0026#34; Description=\u0026#34;Container Variable: PUID\u0026#34; Type=\u0026#34;Variable\u0026#34; Display=\u0026#34;advanced\u0026#34; Required=\u0026#34;true\u0026#34; Mask=\u0026#34;false\u0026#34;/\u0026gt; \u0026lt;Config Name=\u0026#34;PGID\u0026#34; Target=\u0026#34;PGID\u0026#34; Default=\u0026#34;100\u0026#34; Description=\u0026#34;Container Variable: PGID\u0026#34; Type=\u0026#34;Variable\u0026#34; Display=\u0026#34;advanced\u0026#34; Required=\u0026#34;true\u0026#34; Mask=\u0026#34;false\u0026#34;/\u0026gt; \u0026lt;Config Name=\u0026#34;UMASK\u0026#34; Target=\u0026#34;UMASK\u0026#34; Default=\u0026#34;022\u0026#34; Description=\u0026#34;Container Variable: UMASK\u0026#34; Type=\u0026#34;Variable\u0026#34; Display=\u0026#34;advanced\u0026#34; Required=\u0026#34;false\u0026#34; Mask=\u0026#34;false\u0026#34;/\u0026gt; Devices The logic for devices is also very similar, without any special treatment.\n153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 {# Set required devices, gets the name from the name atribute #} {% if param_device_map | default(false) %} {% for item in param_devices %} \u0026lt;Config Name=\u0026#34;{{ ca(item.name) if item.name is defined else item.device_path }}\u0026#34; Default=\u0026#34;{{ item.device_path }}\u0026#34; Description=\u0026#34;{{ ca(item.desc) if item.desc is defined else \u0026#34;Device: \u0026#34; + path }}\u0026#34; Type=\u0026#34;Device\u0026#34; Display=\u0026#34;always\u0026#34; Required=\u0026#34;true\u0026#34; Mask=\u0026#34;false\u0026#34;/\u0026gt; {% endfor %} {% endif %} {#- Set required variables, gets the name from the name atribute #} {#- Set optional devices #} {% if opt_param_device_map | default(false) %} {% for item in opt_param_devices %} \u0026lt;Config Name=\u0026#34;{{ ca(item.name) if item.name is defined else item.device_path }}\u0026#34; Default=\u0026#34;{{ item.device_path }}\u0026#34; Description=\u0026#34;{{ ca(item.desc) if item.desc is defined else \u0026#34;Device: \u0026#34; + path }}\u0026#34; Type=\u0026#34;Device\u0026#34; Display=\u0026#34;always\u0026#34; Required=\u0026#34;false\u0026#34; Mask=\u0026#34;false\u0026#34;/\u0026gt; {% endfor %} {% endif %} {#- Set optional devices #} \u0026lt;/Container\u0026gt; Finishing up The last step is to tie this template into the Ansible play, which I won´t cover here, as it´s not a bodge, like this template is.\n","permalink":"https://nonsense.fyi/posts/templating-templates-with-templates/","summary":"Over at Linuxserver we use Ansible for it\u0026rsquo;s convenient integration with Jinja, a templating engine for Python, along with its powerful framework to execute shell commands. With this we are able to automate quite a lot of work, like generating a uniform and recognizable README, or injecting files into the container(don\u0026rsquo;t worry, the files are committed and pushed to GitHub before the image is built) for some logic. In order for all this to work, most of the metadata is filled into two YAML files in each repository, jenkins-vars.","title":"Templating templates with templates"},{"content":"Now that I have set up this server as I want, the next step is to ensure it stays this way. One of the things I do to achieve this, is by setting up monitoring, and yes, it is very overkill for this setup. I went with a set of tools that I know, with some new additions which has been on my list of things to look into for a while, mainly Loki and Promtail. They are both projects from Grafana which are heavily focused on logs, both presenting them and parse them.\nI\u0026rsquo;m basing this on the TIG(Telegraf, InfluxDB, Grafana) stack, because I didn\u0026rsquo;t want to spend time learning Prometheus, however implementing this took more time than it should\u0026rsquo;ve, Loki even got updated while setting it up. As I said earlier, I wanted to add some new tools in my \u0026ldquo;normal\u0026rdquo; stack. Just because Telegraf can do logs, does not mean it should. It can handle both syslog with a syslog-listener plugin, and docker-logs with the use of the docker socket, but Grafana has no good way of presenting them (I don\u0026rsquo;t blame them).\nSetting up InfluxDB Setting up InfluxDB in docker is quite easy. This is all it takes.\n1 2 3 4 5 6 7 8 9 10 influxdb: image: influxdb:latest container_name: influxdb environment: - TZ=${TZ} ports: - 127.0.0.1:8086:8086 volumes: - /opt/appdata/influxdb/:/var/lib/influxdb restart: unless-stopped Setting up Telegraf Again, I choose to have a service running on the host, rather than in a container. I did this because Fail2Ban run on the host, having a container talking to a service on the host when the Telegraf plugin uses the command-line to interface with the service sounded like an unnecessary challenge.\nMy telegraf.conf is relatively small, only because I added the sections I were going to use, rather than using the one included which has all the options in it. Depending on your experience with Telegraf, it might look a bit complicated, especially the outputs. It is in fact not that advanced.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 [[outputs.influxdb]] database = \u0026#34;telegraf\u0026#34; # required retention_policy = \u0026#34;\u0026#34; write_consistency = \u0026#34;any\u0026#34; timeout = \u0026#34;5s\u0026#34; [outputs.influxdb.tagdrop] influx_database = [\u0026#34;docker\u0026#34;, \u0026#34;nginx\u0026#34;, \u0026#34;system\u0026#34;, \u0026#34;web\u0026#34;] [[outputs.influxdb]] database = \u0026#34;system\u0026#34; # required retention_policy = \u0026#34;\u0026#34; write_consistency = \u0026#34;any\u0026#34; timeout = \u0026#34;5s\u0026#34; tagexclude = [\u0026#34;influx_database\u0026#34;] [outputs.influxdb.tagpass] influx_database = [\u0026#34;system\u0026#34;] This is mainly doing two things, to achieve the goal of diverting each type of metric into their own database in InfluxDB. Having them all in the same one isn\u0026rsquo;t really a problem; it just makes it easier for me. To do this Telegraf has to add a tag to the metric it collects.\n1 2 3 4 5 6 7 [[inputs.cpu]] percpu = true totalcpu = true collect_cpu_time = false report_active = false [inputs.cpu.tags] influx_database = \u0026#34;system\u0026#34; Here I tell Telegraf to tag all the metrics from the cpu input with the tag influx_database set to system. Telegraf can use the tagpass and tagdrop to further filter the metrics. We use tagpass to tell Telegraf which tags this output should allow. In the example I tell it to allow all metrics tagged with influx_database set to system. The tagdrop does the opposite, it tells Telegraf which tags it should not allow. In order to keep the output somewhat clean, I tell Telegraf to not pass the influx_database tag to InfluxDB.\nOther than configuring nginx to show nginx_stub status on 127.0.0.1:8080, the rest of the telegraf.conf is pretty much stock.\n1 2 3 4 5 6 7 server { listen 127.0.0.1:8080; location / { stub_status on; access_log off; } } Setting up Grafana I noticed a while back, that you can configure Grafana purely by using enviroment variables, this is awesome because it can also be set up to not use the embedded sqlite database. Which means I don\u0026rsquo;t need to give it a volume mount for persistent data, I did however give it a mount for using the socket instead of using http between nginx and Grafana.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 grafana: image: grafana/grafana:latest container_name: grafana hostname: ${HOSTNAME} user: \u0026#34;1000:33\u0026#34; environment: - TZ=${TZ} - GF_AUTH_ANONYMOUS_ENABLED=true - GF_AUTH_ANONYMOUS_ORG_NAME=Nonsense - GF_AUTH_ANONYMOUS_ORG_ROLE=Viewer - GF_DATABASE_URL=mysql://grafana:${GrafanaDB_PASS}@mariadb/grafana - GF_INSTALL_PLUGINS=grafana-clock-panel,grafana-worldmap-panel,grafana-piechart-panel - GF_SECURITY_ADMIN_USER=Roxedus - GF_SECURITY_ALLOW_EMBEDDING=true - GF_SECURITY_COOKIE_SECURE=true - GF_SERVER_PROTOCOL=socket - GF_SERVER_SOCKET=/opt/socket/grafana.socket # - GF_SECURITY_ADMIN_PASSWORD=${SQL_ROOT} volumes: - /opt/socket/:/opt/socket I renamed the default organization in Grafana, which means that the default anonymous(guest) access break. I fix that by declaring which organization and role the anonymous user is attached to. Then I tell Grafana how to connect to the database, and what plugins it should install on start. Since I can, I also set the admin username and password, instead of updating the user after the fact.\nUnlike Ghost, Grafana doesnt have a way to set permissions on the socket. Therefore, Grafana runs as my user, as part of the www-data group. It is dirty, but it works.\nReading a simple logfile with Promtail The next part was getting Loki and Promtail set up. Oh boy what a task this was. I don\u0026rsquo;t know why, but there isn\u0026rsquo;t much content about this out there, if I were to guess people run the whole ELK stack. Which is way overkill for what I want to achieve here. The biggest hurdle personally was the fact that it was written in go, and therefore the config used some go-specific rules. Did you know that go\u0026rsquo;s implementation of regex is very minimal? Or that the time-formatting is done without the notion of deliminators like YYYY-MM-DD? I for sure did not.\nGetting the containers running was pretty simple, once you have downloaded the basic configuration files for Loki and Promtail. I choice to not use the default location inside the containers, I followed the practice of having all persistent data inn /config since I can override the config.file location with the command feature of docker.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 loki: image: grafana/loki:${LOKI_VER} hostname: ${HOSTNAME} container_name: loki environment: - TZ=${TZ} volumes: - /opt/appdata/loki/config:/config:ro command: -config.file=/config/loki-config.yaml promtail: image: grafana/promtail:${LOKI_VER} container_name: promtail environment: - TZ=${TZ} depends_on: - loki volumes: - /opt/appdata/promtail:/config:ro - /opt/logs:/opt/logs:ro - /var/log:/var/log:ro - /etc/machine-id:/etc/machine-id:ro command: -config.file=/config/promtail-config.yaml My Loki service is not too far off from the example Loki provides. I added the hostname for the machine and set the TZ. I will come back to the logging part later.\nFor Promtail I gave it two volume-mounts for logs, where /opt/logs is where the logs from the applications I run with Docker are located (not to be confused with docker logs). The next thing I did was to change the promtail-config.yaml file to tell Promtail to read the logs in /opt.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 server: http_listen_port: 9080 grpc_listen_port: 0 positions: filename: /tmp/positions.yaml clients: - url: http://loki:3100/loki/api/v1/push scrape_configs: - job_name: system pipeline_stages: static_configs: - targets: - localhost labels: job: varlogs __path__: /var/log/**/**log - targets: - localhost labels: job: containers __path__: /opt/logs/**/*log I could have added the __path__ to the varlogs job, but I added it as a new job to be able to filter it out easily in Grafana. They would present themselves something like this:\nAs well as labeling the data with the job-name, it also labels it with the filename, which is neat when you want to focus on a specific logfile. If you look closely you can see that Loki didn\u0026rsquo;t read the timestamp properly. This is something we can fix manually and depending on the original time-format its not that difficult, after getting over the hurdles that is go\u0026rsquo;s time-formatting. The documentation does cover it, but it takes a few clicks to get there, so here is a link. There is a catch here though, especially if you know language like python. go\u0026rsquo;s time-formater doesn\u0026rsquo;t handle milliseconds with commas, there is an issue open about this from 2013.\n2021 Note: Looks like this is no longer the case\nYou can still get the timestamp, but there is an additional step that need to be taken for that. I ran into that issue with the fail2ban logs, and I will walk you trough how I solved it. Here is a line from that log.\n2020-05-27 21:51:50,138 fail2ban.filter [9474]: INFO [sshd] Found 1.1.1.1 - 2020-05-27 21:51:50 The first thing that had to be done was extracting the timestamp from the line. This is done with a regex stage. Note the that the backslashes needs to be escaped.\n1 2 3 4 - regex: expression: \u0026#34;(?P\u0026lt;time\u0026gt;\\\\d{4}-\\\\d{2}-\\\\d{2} \\\\d{2}:\\\\d{2}:\\\\d{2},\\\\d{0,3}) (?P\u0026lt;message\u0026gt;fail2ban.*(?P\u0026lt;pid\u0026gt;\\\\[\\\\d*\\\\]: )(?P\u0026lt;level\u0026gt;[A-Z]{4,7}) .*)\u0026#34; I am also going to be extracting a few other things, like the message itself, the PID and the log-level, to use in other stages. You can look at the regex in action on regex101.com.\nNow I can use the content of time group as a variable in Promtail. The next step is to replace the comma with a dot, so Promtail can understand the time-format. This is done with a template step. That is another thing worth looking into, go\u0026rsquo;s template system. After reading about the replace function on the string function, it was pretty clear that it was the last piece of the puzzle I needed to solve before putting it all together.\n1 2 3 - template: source: time template: \u0026#39;{{ Replace .Value \u0026#34;,\u0026#34; \u0026#34;.\u0026#34; -1 }}\u0026#39; Now we have the time variable available, because all the capture groups created with the regex step is exported for us to use. I pass it to the template. .Value is the value from the variable set as source. Then it replaces , with . for -1 times, -1 is the same as replace all. The output of this step is a new time variable, same name, new content. We can now define the timestamp in the next stage.\n1 2 3 - timestamp: source: time format: \u0026#34;2006-01-02 15:04:05.000\u0026#34; This is how you define a time-format in GO. This is it. It is a clever way to do it, it uses a set value for each component. The year it looks for is 2006, so it\u0026rsquo;s either 06 or 2006. The month has a few more options 1, 01, Jan or January. So, in a sense it is all pretty easy to read. All the possibilities for the components are in the documentation for the timestamp stage.\nNow Promtail know what time the log was sent for.\nI am not happy with the result; it has data I don\u0026rsquo;t need, there is more I can do. If you look at the initial line from the log, it also has the time at the end, neither do I need the PID. I can fix this with another template step and a regex step.\n1 2 3 - template: source: message template: \u0026#39;{{ Replace .Value .pid \u0026#34;\u0026#34; -1 }}\u0026#39; In this stage it reads the message extracted from the initial regex, then it replaces the content of the capture-group/label called pid with nothing.\n1 2 3 - regex: expression: \u0026#39;(?P\u0026lt;message\u0026gt;.*)(?: - \\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\u0026#39; source: message This regex step uses the message passed from the previous template step and creates a new message capture-group, the important thing here is that it places the eventual timestamp in its own capture group, which is not a part of the message group.\nIf I were to start Promtail now, it would still see the line as it is in the log. Promtail needs to be told what it should be passing along, so we use the output stage.\n1 2 - output: source: message However it also needs to be told what to apply all these steps to, so a final configuration to get Promtail to read and parse the Fail2Ban log would look something like this.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 server: http_listen_port: 9080 grpc_listen_port: 0 positions: filename: /tmp/positions.yaml clients: - url: http://loki:3100/loki/api/v1/push scrape_configs: - job_name: system pipeline_stages: - match: selector: \u0026#39;{filename=~\u0026#34;.*fail2ban.log\u0026#34;}\u0026#39; stages: - regex: expression: \u0026#34;(?P\u0026lt;time\u0026gt;\\\\d{4}-\\\\d{2}-\\\\d{2} \\\\d{2}:\\\\d{2}:\\\\d{2},\\\\d{0,3})?\\\\D? (?P\u0026lt;message\u0026gt;fail2ban.*(?P\u0026lt;pid\u0026gt;\\\\[\\\\d*\\\\]: )(?P\u0026lt;level\u0026gt;[A-Z]{4,7}) .* (?:(?:\\\\[|Jail \u0026#39;)(?P\u0026lt;jail\u0026gt;\\\\D*)(?:\\\\]|\u0026#39;))?.*)\u0026#34; - template: source: message template: \u0026#39;{{ Replace .Value .pid \u0026#34;\u0026#34; -1 }}\u0026#39; - regex: expression: \u0026#39;(?P\u0026lt;message\u0026gt;.*)(?: - \\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\u0026#39; source: message - template: source: time template: \u0026#39;{{ Replace .Value \u0026#34;,\u0026#34; \u0026#34;.\u0026#34; -1 }}\u0026#39; - timestamp: source: time format: \u0026#34;2006-01-02 15:04:05.000\u0026#34; - output: source: message static_configs: - targets: - localhost labels: job: varlogs __path__: /var/log/**/**log - targets: - localhost labels: job: containers __path__: /opt/logs/**/*log In order for Promtail to know where to apply these stages, it needs to match it to something. Grafana is then again presenting a new concept with Loki, LogQL. It is an adaptation of Prometheus\u0026rsquo; query language, focused on logs. This is used in the selector, I kept mine simple. It is only matching against filenames ending with fail2ban.log, the filename includes paths, which is why there is a wildcard in front.\nReading docker-logs with Promtail Loki has a docker log-driver, which as the time of writing has a few deal breaking issues, mainly #2017. To work around this, I found that sending docker-logs to the journal works for my use case. The Promtail config for this was surprisingly easy, by just setting up the journal job example from Promtails documentation.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 server: http_listen_port: 9080 grpc_listen_port: 0 positions: filename: /tmp/positions.yaml clients: - url: http://loki:3100/loki/api/v1/push scrape_configs: - job_name: system pipeline_stages: - job_name: journal journal: max_age: 12h labels: job: systemd-journal relabel_configs: - source_labels: [\u0026#34;__journal__systemd_unit\u0026#34;] target_label: \u0026#34;unit\u0026#34; That will give us the journal from the host since /var/log/journal is mapped.\nThere is a few ways to tell docker to log to the journal, however I am going to do it in my docker-compose. I simply just add a few lines to the containers I want to see in Grafana.\n1 2 3 4 logging: driver: journald options: mode: non-blocking My Matomo instance would then look something like this.\n1 2 3 4 5 6 7 8 9 10 11 12 matomo: image: matomo:fpm-alpine container_name: matomo logging: driver: journald options: mode: non-blocking depends_on: - mariadb volumes: - /opt/appdata/matomo:/var/www/html - /opt/logs/matomo:/logz Now I can see the logs from the Matomo container in Grafana, however it does not have a label I can use to sort out all the entries which does not come from the container. I can do that by adding a relabel action in the job-definition. Promtail presents all metadata from the journal with the __journal_ label-prefix. This means that we can access the CONTAINER_NAME attribute docker adds to the logfile with the label __journal_container_name, so by adding a relabel_config we can get the container name as a label in Promtail.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 server: http_listen_port: 9080 grpc_listen_port: 0 positions: filename: /tmp/positions.yaml clients: - url: http://loki:3100/loki/api/v1/push scrape_configs: - job_name: system pipeline_stages: - job_name: journal journal: max_age: 12h labels: job: systemd-journal relabel_configs: - source_labels: [\u0026#34;__journal__systemd_unit\u0026#34;] target_label: \u0026#34;unit\u0026#34; - source_labels: [\u0026#34;__journal_container_name\u0026#34;] target_label: \u0026#34;container\u0026#34; It will present the logs similar to this:\n","permalink":"https://nonsense.fyi/posts/deploying-loki-and-promtail-together-with-the-tig-stack/","summary":"Now that I have set up this server as I want, the next step is to ensure it stays this way. One of the things I do to achieve this, is by setting up monitoring, and yes, it is very overkill for this setup. I went with a set of tools that I know, with some new additions which has been on my list of things to look into for a while, mainly Loki and Promtail.","title":"Deploying Loki and Promtail Together With the TIG Stack"}]