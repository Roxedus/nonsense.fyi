[{"content":"As part of a new homeserver build I plan to finish this year, I wanted to look into where the ecosystem is regarding LUKS volumes unlocked by TPM. This was sparked from how seamless it was when I set up my framework last year.\nI gave my self a few conditions for this setup I would like to meet. The first one is Secure boot, it\u0026rsquo;s 2025, I should be able to do this by now, I also became aware of tooling that makes this easier. I would like this setup to use sdboot, for really no particular reason than to try something else than grub. If I could fulfill this, UKI could also be a additional implementation detail.\nI decided to use Ubuntu for this. Ubuntu 24.04 supports setting up LUKS on root as part of its installer, I used that for the initial LUKS setup.\nReplacing grub with sdboot I based this from this Gist comment\nThis also installs the requisites for creating a Unified Kernel Image with TPM support\n1 2 3 4 sudo apt purge -y --allow-remove-essential grub2-common grub-pc-bin grub-pc grub-gfxpayload-lists grub-efi-amd64-bin grub-efi-amd64-signed grub-common os-prober shim-signed libfreetype6 sudo apt-get autoremove -y --purge sudo apt-mark hold \u0026#34;grub*\u0026#34; sudo apt -y install systemd-boot systemd-ukify tpm2-tools libtss2-esys-3.0.2-0 libtss2-rc0t64 dracut The dependency chain for libtss2-esys does not include the required sysusers files to create the tss account in the initramfs. This should have been resolved in this commit, but I can only assume the configure script used in debian/ubuntu is not set up to output these (which Arch has done). Therefore we have to grab this manually.\n1 sudo curl -o /usr/lib/sysusers.d/tpm2-tss.conf https://raw.githubusercontent.com/tpm2-software/tpm2-tss/refs/heads/master/dist/sysusers.d/tpm2-tss.conf Now it\u0026rsquo;s time to reboot, we should now be booting with sdboot, as the systemd-boot package also installed the bootloader into the EFI partition previously used by grub. We can confirm the bootloader with bootctl, looking at the product under the current boot loader entry.\nSecure Boot With a touch of FoxWare we can easily achieve Secure Boot. The goal here is to run our own secure boot certificate, and sign the kernel ourself.\nI use sbctl for this, as it makes the whole setup painless. It is not currently packed for the default debian/ubuntu repositories, but it is getting there. There is however a community package available in the meantime.\n1 2 3 4 5 6 7 echo \u0026#39;deb http://download.opensuse.org/repositories/home:/jloeser:/secureboot/xUbuntu_24.04/ /\u0026#39; | sudo tee /etc/apt/sources.list.d/home:jloeser:secureboot.list curl -fsSL https://download.opensuse.org/repositories/home:jloeser:secureboot/xUbuntu_24.04/Release.key | gpg --dearmor | sudo tee /etc/apt/trusted.gpg.d/home_jloeser_secureboot.gpg \u0026gt; /dev/null sudo apt update sudo apt install -y sbctl # This is just to ensure the config file exist in the expected location sudo mkdir -p /etc/sbctl sudo touch /etc/sbctl/sbctl.conf With sbctl installed, it is now time to create and enroll the needed keys for secure boot. This is just two commands with sbctl. This assumes that your motherboard is already configured to be able to receive custom keys\n1 2 sudo sbctl create-keys sudo sbctl enroll-keys At this point we can run sbctl verify to get a list of entries detected in the bootloader. It should also note these are not currently signed. Now that we have enrolled keys, secure boot should not be in setup mode, and thus require images to be signed.\nI like to sign all entries in the bootloader at this stage. I usually do that by assuming all entries are in a couple of directories in /boot.\n1 2 3 for file in /boot/efi/*/*/linux /boot/efi/EFI/*/*.efi; do sudo sbctl sign -s $file done After verifying these are signed, it should now be safe to reboot. In this reboot, it is worth checking with the uefi that secure boot is enabled.\nUnified Kernel Image Now that we have secure boot, as well as sdboot going, we can implement booting with a UKI. This allows us to use some of the measurements UKI booting does, to tie into cryptsetup and automatic LUKS unlocks.\nThe steps to make this happen differs a bit between versions. The difference might just boil down to me missing something, but my untested theory is that some package has moved the heavy lifting to kernel-install, as 24.10 also automatically signs the image with sbctl as well as sorting out the bootloader entry.\nWhile writing this, I used both Ubuntu 24.10 and 24.04.\nFor both versions, I told Dracut to run with hostonly.\n1 2 3 cat \u0026lt;\u0026lt; EOF | sudo tee /etc/dracut.conf.d/hostonly.conf hostonly=\u0026#34;yes\u0026#34; EOF For 24.10, only kernel-install needs to be told to create a UKI.\n1 2 3 cat \u0026lt;\u0026lt; EOF | sudo tee /etc/kernel/install.conf layout=uki EOF For the current LTS, we need to tell Dracut to create a UKI with the option --uefi.\n1 2 3 cat \u0026lt;\u0026lt; EOF | sudo tee /etc/dracut.conf.d/uefi.conf uefi=\u0026#34;yes\u0026#34; EOF As mentioned above, my experience is that there is a couple of manual tasks left to do.\nSign the UKI\nTell the bootloader to use the UKI as the default entry.\nYou can do this with bootctl set-default and pointing it to the entry matching the name Dracut built.\nNow we add TPM support to the UKI, by telling Dracut to include the tpm2-tss module, as well as forcing a build.\n1 sudo dracut -f --add tpm2-tss Verify that the image is signed before rebooting. To verify the UKI was used to boot, we can use bootctl and check that TPM2 Support and Measured UKI are true.\nTPM Backed LUKS For this writeup I used somewhat relaxed measurements. The explanations are fetched from UAPI.\n0: Core system firmware executable code 2: Extended or pluggable executable code; includes option ROMs on pluggable hardware 7: SecureBoot state While testing I used blkid to fetch the mount path or UUID of a device with LUKS set up. This substitution works if there is only one LUKS device in the system.\nWe can now enroll TPM2 as a keyslot for the LUKS partition.\n1 sudo systemd-cryptenroll $(sudo blkid -o device --match-token TYPE=crypto_LUKS) --tpm2-device=auto --tpm2-pcrs=0,2,7 Since we use Dracut, we also need to set the crypttab. This line should probably be a sed, but while writing, the Ubuntu installer always set up the LUKS volume with the same name, and this was just easier.\n1 2 3 cat \u0026lt;\u0026lt; EOF | sudo tee /etc/crypttab dm_crypt-0 UUID=$(sudo blkid -o value -s UUID --match-token TYPE=crypto_LUKS) none tpm2-device=auto,luks,tpm2-pcrs=0+2+7 EOF We should now be able to reboot, without being prompted by the LUKS password.\nUpdates Depending on the measurements used, you might have to bind the keyslot again, with the new values of the measurement.\nBIOS updates would change the value of PCR0, and thus require a setting the keyslot again.\nThe command is quite similar to the original enrollment, with the addition of telling it to wipe the previous tpm slot.\n1 sudo systemd-cryptenroll $(sudo blkid -o device --match-token TYPE=crypto_LUKS) --tpm2-device=auto --tpm2-pcrs=0,2,7 --wipe-slot=tpm2 ","permalink":"https://nonsense.fyi/posts/setting-up-tpm2-backed-luks-at-root-with-secure-boot-in-ubuntu/","summary":"\u003cp\u003eAs part of a new homeserver build I plan to finish this year, I wanted to look into where the ecosystem is regarding LUKS volumes unlocked by TPM. This was sparked from how seamless it was when I set up my framework last year.\u003c/p\u003e\n\u003cp\u003eI gave my self a few conditions for this setup I would like to meet.\nThe first one is Secure boot, it\u0026rsquo;s 2025, I should be able to do this by now, I also became aware of tooling that makes this easier.\nI would like this setup to use sdboot, for really no particular reason than to try something else than grub. If I could fulfill this, UKI could also be a additional implementation detail.\u003c/p\u003e","title":"Setting up TPM2 backed LUKS at root with secure boot in Ubuntu"},{"content":"So I took some certs Ive recently gone trough the Kubernetes Administrator, Developer and Security Specialist certifications. In typical fashion I broke some stuff on my way to get there.\nThe git references in this article are not up-to-date, as later deployments has made me consolidating multiple repos to a single infra repo. Preparation This is everything I used to prepare for these certifications:\nTime Some smol computing boys, three Raspberry PI 4 8GB was hurt for this blogpost Lectures with practical tests, I went with the courses from KodeKloud A not so big LXC running on Proxmox 7 Patience Time Proxmox and LXC For some unrelated infrastructure, I set up Proxmox. I attempted to put most of the configuration in Ansible, but its just not feasible in the long run. The Proxmox host is however not fully ClickOps based, as packages, swap and the certificate(fetched from OpnSense) is managed by the playbook.\nControlplane LXC Because sourcing additional Raspberry PI\u0026rsquo;s was an impossible task (and still isn\u0026rsquo;t easy), I thought \u0026ldquo;Hey, my Proxmox host is only doing Home-Assistant right know, I can squeeze more onto it\u0026rdquo;.\nSince I am who I am, I decided to dabble in something new, enter Linux Containers(lxc)1.\nThe reason for researching this method is straightforward; VMs are heavy, OCI2 containers are light. I needed something in between. This host is limited on resources, but not starved for them. Therefore avoiding running another kernel and subsystem would be preferred, this ruled out Virtual Machines. Another option I am very comfortable with, is Docker. However, (stock) Kubernetes really wants a fully-fledged init system running and I am not crazy enough to run a complete Systemd instance in a Docker container.\nAs it took multiple attempts to get the LXC configured to make kubeadm happy, I saved that file once all the issues was ironed out. It looks something like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 arch: amd64 cores: 2 hostname: controlplane memory: 4096 net0: name=eth0,bridge=vmbr0,firewall=1,gw=\u0026lt;gateway ip\u0026gt;,hwaddr=\u0026lt;mac\u0026gt;,ip=\u0026lt;ip with cidir range\u0026gt;,type=veth ostype: ubuntu rootfs: \u0026lt;DiskInfo\u0026gt;,size=50G searchdomain: kube.\u0026lt;domain\u0026gt; swap: 0 features: fuse=1,mount=nfs,nesting=1 lxc.apparmor.profile: unconfined lxc.cap.drop: lxc.cgroup.devices.allow: a lxc.mount.auto: proc:rw sys:rw I later tried to define this LXC with Terraform, but ran mostly into auth issues. This LXC needed to be privileged, which means it cannot be created by anyone else then the \u0026ldquo;root\u0026rdquo; user, making a service account useless. I have also opted for using MFA on the root account, which made user/password moot.\nPlaying with the playbook As mentioned, I have a Ansible playbook for infrastructure running inside my lan. This playbook gives me a familiar and uniform environment for all hosts. It handles common packages I have come to expect being present on the machine im working on, while setting up my user with a public key, and the prompt in my shell.\nTo get my environment ready, I wrote a Kubernetes role into my existing Ansible Infra playbook, this takes care of both Kubeadm\u0026rsquo;s and Kubernetes\u0026rsquo; requirements. This includes disabling swap, setting up containerd as a runtime(thanks Jeff) and a whole lot of kernel tuning. Using some roles in the Ansible Galaxy might have saved me some time here, but before I landed on using Jeff\u0026rsquo;s containerd role, I tried to use some of the cri-o roles, but I spent too much time digging in apt repos to mitigate the fact that Kubernetes with ARM64 nodes is less adopted than I thought(more on this later). I quickly abandoned the idea of using a Galaxy role for cri-o, after struggling to get cri-o going at all, I gave up on cri-o altogether and settled for the containerd role.\nSince my lab is run on somewhat unconventional setups, I had to make several changes for Raspberry PIs such as enabling Cgroups and setting the GPU memory size. As well for some enabling KMSG in the LXC. While all this was working fine in its current state, I wanted to move this lab to a vlan, to segment lab ip range from the proper lan(using a vlan with crosstalk, so no isolation). In Proxmox this is a few straightforward checkboxes, and a textfield, for the Raspberries, this would become harder than it needed to be. In Ubuntu 21.10 they removed the linux-modules-extra-raspi package from the base image, this meant that the kernel module 802.1q was not able to be loaded, and thus no vlan. I did a digression into HashiCorp\u0026rsquo;s Packer to determine if it would be worth it to bundle this myself, it wasn\u0026rsquo;t. While I really wanted the playbook to do it all, it now meant I had to ssh into the two nodes after the initial playbook run. I did help myself here, as I bootstrapped the Raspberries with a network-config file already set up with the wanted network configuration, all I had to do after the initial run was to alter this file on the host, setting dhcp to false on the untagged interface.\nCould I have done this in Ansible? yes. Did I want to? no, I already locked myself out too many times writing this playbook.\nKubeadm init Now that the nodes OS is provisioned, and kubelets are running, it was time to set up the kubernetes cluster. I choose to go with Kubeadm for bootstrapping the cluster, simply because it\u0026rsquo;s easier than \u0026ldquo;the hard way\u0026rdquo;, and tested in production systems.\nBefore running the command, you need to make some decisions on networking, as certain solutions need Kubeadm to make changes to its configuration.\nSince the LXC is going to be the controlplane, that\u0026rsquo;s where I initialize the cluster. Proxmox\u0026rsquo;s kernel isn\u0026rsquo;t fully compatible with the requirements Kubeadm has set, so the cluster had to be initialized by bypassing the check for the config kernel setting.\n1 [ERROR SystemVerification]: failed to parse kernel config: unable to load kernel module: \u0026#34;configs\u0026#34;, output: \u0026#34;modprobe: FATAL: Module configs not found in directory /lib/modules/5.15.30-2-pve\u0026#34;, err: exit status 1 The command I ended up using was\n1 kubeadm init --pod-network-cidr=10.244.0.0/16 --ignore-preflight-errors=SystemVerification In the output of the init command, I made a note of the join command. The output also tell you that now time is the time to get a CNI3, once again I went the easy route with Flannel, as it just works out of the box. For my next iteration I am considering Weave, due to it supporting network policies.\nThe output helpfully gives you a copy-paste of the commands you need to do if you want to interface with the cluster outside of the root account, the file in this example is also the file you want to have on the computer you plan to manage the cluster by(or you can already here look at creating a dedicated certificate, rather than the one Kubeadm generates).\nThe next step was running the command with the join token I noted earlier on the other nodes, this should be a painless process. It was, I was now watching the command kubectl get nodes, and looking at all the nodes getting to a ready state, it took some convincing, but after a couple of test pods also going into ready state I now had a working cluster.\nGitOps Because I also had video lectures with practice tests I was doing at the same time, I were already comfortable with kubectl and dealing with bare manifests. I had no desire to do that in this lab, I therefore dove head first into Helm4 and Argo CD5.\nThis section is going to be a bit messy, as I will be linking different commits, from a git-tree that has been rewritten( to remove my previous attempt at trying this, having no idea what I was doing). This is why the first commit has a structure, and a bunch of files. Getting started I installed the Helm binary and went to town. I created a namespace for Argo, then I installed it trough the chart, specifying the namespace. My next step was looking into how I could have Argo manage itself, I quickly discovered the \u0026ldquo;App of apps\u0026rdquo; pattern, realizing my initial deployment could have been done smoother. I then tore down the deployment and followed the Helm example in Argos docs, creating the subchart(if you for some reason are following along, you need to pull down the chart locally before you can install it). The install command is now a bit different, as I am using a local chart folder.\n1 helm install argo-cd Charts/argo-cd/ --namespace argo-cd This chart has some values in values.yaml I remember struggling with in my first attempt. I disable dex, as I have no need for external authentication to Argo at the moment. I told the chart to change the service type to nodeport, mainly because I didn\u0026rsquo;t feel like setting up the kubectl port-forward each time I want to look at it. I created another admin account with the Roxedus username (to set the password, you need to use the argo cli/api to reset it with another admin account). As I have no method to generate tls certificates, Argo gets the --insecure argument which tells it to allow unencrypted access to the page. Note the comment at the bottom, in my first attempt at this, Argo had some issues with it\u0026rsquo;s pipeline regarding building and pushing for arm64.\nThe next commit does a couple of things, but I am mainly focusing on apps subfolder for the moment. In this folder, I define the root application, thanks to Argos application custom resource. The important bit here, is apps/templates/argo-cd.yaml, it tells Argo to create a internal application, to track a subfolder in git, this is the same git repository in which the ArgoCD helm subchart lives. Once I tell Argo to watch this apps folder, I can do all creation and destruction of deployments trough git. There is a couple of ways to tell Argo on how to watch this repository, you can use kubectl to apply a manifest, using the argocd cli, or the easy way, trough the webui. There wasn\u0026rsquo;t much info needed to fill here, just the repo url, and optionally a subfolder. To keep some order, I decided to keep all application definitions in a apps folder. Once this was created, I immediately saw it pick up the Argocd chart, and it started to do a couple of remediations, as it had deviated a bit from the initial install.\nKeeping updated In the same commit as the one deploying the Argo chart, there is also a cronjob definition for Renovatebot, which is a tool for checking a git repository for new version numbers. As the Argo subchart is using a specific version tag, and Renovate speaking Helm repos, Renovate is able to see that there is a new version. It then creates a pull-request on my git repo to update this file to the new version.\nThe image Renovate publish as default has all the tooling it needs in order to parse and understand all the package systems it knows, this makes it very large, the image is 1.3 GB compressed. The alternate slim tag only contains node, and relies on the ability to spin up additional containers in order to load tooling. Because it holds all the tooling, it also makes it unwieldy to offer in multiple architectures, I needed to allow it to run on the one amd64 machine in this cluster, the controlplane. Therefore the job is set to run on nodes with the built-in architecture label, however this is not enough, as controlplanes are by default not allowed to run workloads, you need to tell the workload it is allowed to run on the controlplane, this is done with tolerations.\nThis cronjob got deployed in a \u0026ldquo;CI\u0026rdquo; application in Argo, because I figured having a dedicated namespace for CI might be beneficial in the future.\nUnlike linitng-tools and other project-specific tools, Renovate is quite flexible with the location and naming of its project-specific configuration, I choose to call it .renovaterc. This file evolved a little of the span of this journey, but it mostly stayed the same, set to watch over pure Kubernetes manifests, Helm charts and Argo-cd.\nIt is very easy to track changes, and updates I have approved(merged) or declined(closed) in Gitea.\nAdditional Infrastructure I consider many of the objects in this section as \u0026ldquo;meta-objects\u0026rdquo;, objects that needs to exist, but does not directly tie into a deployment or application.\nConnecting people Any good homelab these days will result in some webguis you may want to use in order to keep in track with the current state of the lab, this one is no different. This desire spawned many questions and failed solutions, yet trough all the failed attempts, I managed to keep Traefik as a constant in this endeavour.\nThe first and simple solution I had in mind was using Cloudflare tunnels to handle external traffic. This was working, but relied on me manually creating tunnels, and cloudflare being in charge of managing TLS, which I don\u0026rsquo;t love.\nThe next solution is based on a neat project, justmiles/traefik-cloudflare-tunnel. It does exactly the steps I previously did manually, but this does it programmatically, and by reading the Traefik routes. Being on arm64 really started hurting here, as cloudflared6 at the time did not build images for arm (Traefik-cloudflare-tunnel still doesn\u0026rsquo;t, February 2023). This lead to another tangent, as I cobbled together some build stuff using Github Actions and QEMU to build these projects for arm, all while not modifying the source. This was all done using docker-bake, in my pipelines repo.\nNone of these solutions is using the ingress mechanism in Kubernetes, so I kept on looking.\nBeing inspired by this post by TheOrangeOne, I decided to try rearchitecting the reverse-proxy solution running on my mediaserver, to be fronted by HAProxy for the SNI routing abilities, but I just couldn\u0026rsquo;t befriend using the proxy protocol in nginx.\nAfter coming to the realization that I will always be connected to my lan, thanks to Wireguard, I came to the conclusion of not needing to get to the cluster from the outside. This opened a new avenue of trial and error, mostly to get Traefik to listen to port 80 and 443. I went into this challenge knowing the prerequisites to get this going in Docker and plain Debian. To do this in Kubernetes, you need to tell the pod to attach to the host network, as well as to tell the process to run as root and setting sysctls. I used this setup for a while, but I was not happy having to deal with using host networking. As a side-note this also had me labeling nodes which had the dns set up.\nLittle loadbalancer that could The final solution depends on Metallb to do the heavy work. If your cluster is not in a cloud, this will do wonders for your load-balancer woes. It has a couple of working-modes, BGP or ARP(layer 2), in my lab it is working with ARP, as none of the listed limitations applies to my use-case. I set it up with 9( or is it 10?) IPs. Once this was setup, I was now able to create loadbalancer services in my cluster. This allowed me to revert the changes making Traefik running as root, and all the other changes I needed previously.\nSaving to disk One of my biggest gripe about kubernetes (or any distributed compute in general) is storage, for many of the applications you want to run, NFS is suitable, however a decent amount of the applications I am interested to eventually run in a cluster does not work well with NFS. There is a lot of solutions to this, some use a host path and lock the pod to that node, others still want to use NFS, I went for Longhorn.io which replicates files in mounts across hosts, allowing pods to migrate between nodes. Longhorn exposes a StorageClass which is a native kubernetes concept I can deal with.\nManaging certificates While I relied on Traefik\u0026rsquo;s way to handle and generate certificates, and had no problems with this, I was looking into more and more potential applications that works against the tls type of kubernetes secrets. This type was something I had in mind after looking into Traefik\u0026rsquo;s IngressRoute CRD against it\u0026rsquo;s kubernetes ingress integration. At this point I also told myself I was done using shortcuts like IngressRoute when theres built-in functionality.\nThe helm chart itself is pretty standard, I didn\u0026rsquo;t have to specify much, just some Cloudflare-specific dns stuff(This has to be a LeGo thing, as this also needed for Traefik), all the configuration I needed was provisioning a Cloudflare-issuer, you tell this spec which secret it should get the api token from.\nLike any sane person, I thought testing against the very application I need to revert the change was the best fit, so I went ahead and enabled ingress as well as cert-generation at the same time to Argo, while hoping this didn\u0026rsquo;t lock me out.\nI checked the status of the certificate request and order this triggered, to see if it generated a certificate, and sure enough, the order was fulfilled and Traefik used the certificate to serve the Argo subdomain.\n1 2 3 $ kubectl get certificaterequests.cert-manager.io -n argo-cd NAME APPROVED DENIED READY ISSUER REQUESTOR AGE argo-roxedus-com-cert-qzwf2 True True roxedus.com-cloudflare system:serviceaccount:cert-manager:cert-manager 7d 1 2 3 $ kubectl get orders.acme.cert-manager.io -n argo-cd NAME STATE AGE argo-roxedus-com-cert-qzwf2-3069573698 valid 7d Keeping secrets Kubernetes secrets are stored in etcd as plaintext by default, which in my lab isn\u0026rsquo;t really that big of a deal, but it is something I wanted to prevent if I could. Before starting on this adventure I have always wanted to get some hands-on with HashiCorp Vault, so it was the obvious choice when I needed a secrets manager. Deployment was a breeze, just tell the chart what type of storage class to use, and we are of to the races. I then configured Vault to use kubernetes as a authentication method with short lived tokens.\nI could have used Vault\u0026rsquo;s injector, but I choose to look for a solution that presents native kubernetes objects in the end, this way I can introduce new applications with less changes to helm charts. This is where external-secrets.io comes into play. Much like the Cert Manager chart, there was not much needed in the helm values, as most configuration happens in dedicated manifests.\nFor external-secrets to create a kubernetes secret, one would need to create a ExternalSecret object telling the operator which key and property the secret has in Vault, and to which name the kubernetes secret should have, and optionally which namespace this should target. The ExternalSecret require a SecretStore (or optionally a ClusterSecretStore) to tell the operator about the external secret manager, and how to authorize against the manager, which in this case is trough the short lived tokens.\nMy first deployment Although SearXNG is the first deployment I did outside of helm, it took shape over multiple iterations, as my portfolio of meta-objects evolved. The first version of this used only a NodePort service and a ConfigMap to accompany the deployment. It\u0026rsquo;s history is a good representation of how this whole cluster evolved, as I used this as my test application. It includes changes like using the Traefik IngresRoute CRD to migrating to Ingress.\nCome certification After finishing the courses on KodeKloud, as well as building this cluster, I was content with my skills against the outlined areas in the CKA curriculum, and went ahead to do a run in KillerShell(you get two runs in this simulator with the purchase of the exam trough Linux Foundation). It\u0026rsquo;s a good thing this simulator is supposed to be more challenging than the exam, because I was starting to question my skills, regardless, I went ahead and scheduled the exam for a friday, as this was a slow day in my calendar. When I exited the room after submitting my answers, I did not have high hopes, but was still exited to get the result within 24 hours, worst case I could schedule my included second attempt.\nWhile minding my own business, cleaning my apartment, the email came, I passed the exam! I was surprised I managed to land this without much \u0026ldquo;real world\u0026rdquo; experience.\nI started the following week being quite happy with the achievement from last week, I had set a goal to get CKA and CKAD done by March, and was half-way already in January. We were multiple people at the office going trough different certifications, since the atmosphere still was set on certifications, I figured I should at least look at the curriculum for CKAD, and they looked quite manageable. On Tuesday I was back on the course-grind, by Thursday I had started a run on KillerShell, this time for CKAD.\nOn Monday I scheduled the CKAD exam for the following day, I was quite confident, as the curriculum overlaps a great deal with the CKA, and the fact that I had unknowingly done a lot of the tasks the CKAD focuses on in my own cluster. Tuesday went, and my confidence were still present while leaving the room.\nThe wait for this email was a bit more nerveracking, mostly because I couldn\u0026rsquo;t be irresponsible by gaming/sleep trough most of the hours, like I could over the weekend. The overall wait time also ended up being a while longer, but it finally came. I passed this one too! I almost expected to pass this one, but it was very comforting receiving confirmation.\nNow work started picking up pace, so I had to shift my focus towards other stuff for a while.\nAs the summer vacation began to close in, work slowed down, I could now focus some more on certifications.\nI had one goal for the summer, which was CKS, I was mentally prepared for a month of heavy, and probably demotivating learning. For this certification I also decided to stick to KodeKlouds courses, mainly because their labs resonate with my way of learning. My expectations were quickly proved wrong, as I found most of the topics interesting, or it brought up scenarios I already have thought about, and solved (ie. SSH key-auth). This course also made me understand AppArmor, which I had brushed off years ago as very advanced.\nI spent a couple of weeks on going trough the courses and labs, before I adventured into KillerShell, where I once again was happy with my results. I scheduled and took the exam the same week as I went trough KillerShell.\nWhile I skipped a whole task in the exam, I still managed to pass this too.\nLinux Containers is an operating-system-level virtualization method for running multiple isolated Linux systems (containers) on a control host using a single Linux kernel. Wikipedia\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nInitiative created by Docker to define standards for multiple aspects regarding running containers. OpenContainers\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCalled Network Plugin in the Kubernetes docs, a component that handles networking between pods.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe package manager for Kubernetes. helm.sh\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nArgo CD is a declarative, GitOps continuous delivery tool for Kubernetes. argoproj.github.io\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSoftware responsible for receiving the traffic from a Cloudflare tunnel. Github\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://nonsense.fyi/posts/road-to-ckad/","summary":"\u003ch2 id=\"so-i-took-some-certs\"\u003eSo I took some certs\u003c/h2\u003e\n\u003cp\u003eIve recently gone trough the Kubernetes Administrator, Developer and Security Specialist certifications. In typical fashion I broke some stuff on my way to get there.\u003c/p\u003e\n\n\u003cdiv class=\"admonition note\"\u003e\n    \n    \u003cdiv class=\"content\"\u003eThe git references in this article are not up-to-date, as later deployments has made me consolidating multiple repos to a single infra repo.\u003c/div\u003e\n\u003c/div\u003e\n\u003ch2 id=\"preparation\"\u003ePreparation\u003c/h2\u003e\n\u003cp\u003eThis is everything I used to prepare for these certifications:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTime\u003c/li\u003e\n\u003cli\u003eSome smol computing boys, three Raspberry PI 4 8GB was hurt for this blogpost\u003c/li\u003e\n\u003cli\u003eLectures with practical tests, I went with the courses from KodeKloud\u003c/li\u003e\n\u003cli\u003eA not so big LXC running on Proxmox 7\u003c/li\u003e\n\u003cli\u003ePatience\u003c/li\u003e\n\u003cli\u003eTime\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"proxmox-and-lxc\"\u003eProxmox and LXC\u003c/h3\u003e\n\u003cp\u003eFor some unrelated infrastructure, I set up Proxmox. I attempted to put most of the configuration in Ansible, but its just not feasible in the long run. The Proxmox host is however not fully ClickOps based, as packages, swap and \u003ca href=\"https://git.roxedus.dev/Roxedus/Infra/src/commit/946284e0bed7a334949755be45cfcb9b55a7e881/ansible/roles/proxmox/templates/get_cert.sh.j2\" target=\"_blank\" rel=\"noopener\"\u003ethe certificate(fetched from OpnSense)\u003c/a\u003e is managed by the playbook.\u003c/p\u003e","title":"Road to CKAD"},{"content":"Over at Linuxserver we use Ansible for it\u0026rsquo;s convenient integration with Jinja, a templating engine for Python, along with its powerful framework to execute shell commands. With this we are able to automate quite a lot of work, like generating a uniform and recognizable README, or injecting files into the container(don\u0026rsquo;t worry, the files are committed and pushed to GitHub before the image is built) for some logic. In order for all this to work, most of the metadata is filled into two YAML files in each repository, jenkins-vars.yml and readme-vars.yml which are then presented to Ansible as variables used for consumption in the templates.\nWhy? Having all the metadata stored in a couple of files that can be pragmatically read makes it relatively easy to expand what we are able to output, and ensuring changes propagates trough all outputs.\nAside from the Dockerfiles, the mentioned var files, and most of the root/ folder, the rest of the repository are actually templated. The benefit of this approach is huge, as we often only need to update a file once, and the changes are done in the repositories as they receive updates, like when we enabled and started promoting GHCR as the default registry for all our images, with a single pull request.\nIf you are used to the(now paid) feature of DockerHub, rebuilding images when the repository updated on GitHub, one would be accustomed to the idea of the readme on DockerHub to always reflect the readme on GitHub, however that is not the case, regardless if the repositories are linked. It is traditionally a thing you have to update by hand, however with some creative thinking you can update this with code, which we do. This means that the readme on GitHub and DockerHub is always up-to-date and identical(as long as it\u0026rsquo;s not to long for DockerHub). There\u0026rsquo;s also a derivative from the readme published to the documentation site for each image, going more in-depth for sections of the readme. To up the inception scale, we also template the CI/CD pipeline, from the small stuff like the greetings bot to the whole Jenkinsfile used to build, test and push the images.\nRemembering the once-in-a-while tasks As I touched on in the blog post announcing automated Unraid templates, creating templates for Unraid was a manual task, often depending on someone in the Linuxserver team using Unraid to actually creating one, this could mean it had the potential to take days or even weeks to push a template. As there is a decent amount of tasks tied to launching a image it might even be forgotten, so automating this step would be better for everyone involved.\nHow? As outlined earlier the important building-blocks are present, a templating engine and repository-level metadata, despite this I had to create some new blocks.\nThis adventure started with getting reacquainted to XML, as that\u0026rsquo;s how the Unraid templates are stored, remembering the specification will surely help with some future headaches.\nGetting started I start by making some helpful notes to any potential contributor wanting to help us maintaining the template, pointing them to the correct file for changing the output. As this is done in the template, the full address for the readme-vars.yml file will point to the actual repository.\n40 41 42 \u0026lt;?xml version=\u0026#34;1.0\u0026#34;?\u0026gt; \u0026lt;!-- DO NOT CHANGE THIS FILE MANUALLY, IT IS AUTOMATICALLY GENERATED --\u0026gt; \u0026lt;!-- GENERATED FROM {{ project_github_asset }}/readme-vars.yml --\u0026gt; Simple string substitution and conditionals Now it\u0026rsquo;s time to create the stuff that actually matters for Unraid, this is stored under the Container tag in the XML file.\n43 44 45 46 47 48 49 50 51 52 53 54 55 56 \u0026lt;Container version=\u0026#34;2\u0026#34;\u0026gt; \u0026lt;Name\u0026gt;{{ param_container_name | lower }}\u0026lt;/Name\u0026gt; \u0026lt;Repository\u0026gt;lscr.io/{{ lsio_project_name_short }}/{{ project_name }}\u0026lt;/Repository\u0026gt; \u0026lt;Registry\u0026gt;https://github.com/orgs/{{ lsio_project_name_short }}/packages/container/package/{{ project_name }}\u0026lt;/Registry\u0026gt; \u0026lt;Network\u0026gt;{{ param_net if param_usage_include_net is sameas true else \u0026#39;bridge\u0026#39; }}\u0026lt;/Network\u0026gt; \u0026lt;Privileged\u0026gt;{{ \u0026#34;true\u0026#34; if privileged is sameas true else \u0026#34;false\u0026#34; }}\u0026lt;/Privileged\u0026gt; \u0026lt;Support\u0026gt;{{ project_github_repo_url }}/issues/new/choose\u0026lt;/Support\u0026gt; \u0026lt;Shell\u0026gt;bash\u0026lt;/Shell\u0026gt; \u0026lt;ReadMe\u0026gt;{{ project_github_repo_url }}{{ \u0026#34;#readme\u0026#34; }}\u0026lt;/ReadMe\u0026gt; \u0026lt;Project\u0026gt;{{ project_url }}\u0026lt;/Project\u0026gt; \u0026lt;Overview\u0026gt;{{ ca(project_blurb) | trim }}\u0026lt;/Overview\u0026gt; \u0026lt;GitHub\u0026gt;{{ project_github_repo_url }}{{ \u0026#34;#application-setup\u0026#34; if app_setup_block_enabled is defined and app_setup_block_enabled }}\u0026lt;/GitHub\u0026gt; \u0026lt;TemplateURL\u0026gt;{{ \u0026#34;false\u0026#34; if unraid_template_sync is sameas false else \u0026#34;https://raw.githubusercontent.com/linuxserver/templates/main/unraid/\u0026#34; + project_name | lower + \u0026#34;.xml\u0026#34; }}\u0026lt;/TemplateURL\u0026gt; \u0026lt;Icon\u0026gt;https://raw.githubusercontent.com/linuxserver/docker-templates/master/linuxserver.io/img/linuxserver-ls-logo.png\u0026lt;/Icon\u0026gt; There is a few things happening here, mostly normal substituting of variables, there is also some transforming done, as \u0026quot;\u0026quot; is not a valid value and literal booleans needed to be it\u0026rsquo;s string counterpart, along with some logic to conditionally append a link. The rest of the template consists mostly of these types of substitutions and transformations. We will get to ca() later\nGoing in loops Unraid templates support multiple branches. When installing from a template with multiple branches defined using Community Applications, you will get prompted with a selection box with all the branches listed in the template. To populate these fields, I iterate from the same variable that lists the branches on the readme, however I have recently added some filtering here to avoid listing deprecated branches.\n59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 {# Set the Branches, if any Config items is overwritten. TODO: handle config items #} {% if development_versions is defined and development_versions == \u0026#34;true\u0026#34; %} {% for item in development_versions_items if not \u0026#34;deprecate\u0026#34; in item.desc.lower() %} \u0026lt;Branch\u0026gt; \u0026lt;Tag\u0026gt;{{ ca(item.tag) }}\u0026lt;/Tag\u0026gt; \u0026lt;TagDescription\u0026gt;{{ ca(item.desc) }}\u0026lt;/TagDescription\u0026gt; {% if item.tag != \u0026#34;latest\u0026#34; %} \u0026lt;ReadMe\u0026gt;{{ project_github_repo_url }}{{ \u0026#34;/tree/\u0026#34; + item.tag + \u0026#34;#readme\u0026#34; }}\u0026lt;/ReadMe\u0026gt; \u0026lt;GitHub\u0026gt;{{ project_github_repo_url }}{{ (\u0026#34;/tree/\u0026#34; + item.tag + \u0026#34;#application-setup\u0026#34;) if app_setup_block_enabled is defined and app_setup_block_enabled }}\u0026lt;/GitHub\u0026gt; {% endif %} {% if item.extra is defined %} {#- Allow for branch-specific stuff #} {{ ca(item.extra) | indent(8) | trim }} {% endif %} \u0026lt;/Branch\u0026gt; {% endfor %} {% endif %} {# Set the Branches, if any #} This snippet is just a simple loop going over the development_versions_items list of arrays if development_versions exists. The following readme-vars.yml produced the above screenshot:\n1 2 3 4 5 6 7 # development version development_versions: true development_versions_items: - { tag: \u0026#34;latest\u0026#34;, desc: \u0026#34;Stable Radarr releases\u0026#34; } - { tag: \u0026#34;develop\u0026#34;, desc: \u0026#34;Radarr releases from their develop branch\u0026#34; } - { tag: \u0026#34;nightly\u0026#34;, desc: \u0026#34;Radarr releases from their nightly branch\u0026#34; } - { tag: \u0026#34;nightly-alpine\u0026#34;, desc: \u0026#34;Radarr releases from their nightly branch using our Alpine baseimage\u0026#34; } I took the opportunity to add a key called extra for the dictionary, as CA has the ability to have separate config variables per branch. Unfortunately this is implemented in a way which makes it hard to use, the presence of any branch-specific items disregards all other config tags specified in the Container tag. Meaning that a dictionary like { tag: \u0026quot;nightly\u0026quot;, desc: \u0026quot;Radarr releases from their nightly branch\u0026quot;, extra: { nightly_var: \u0026quot;Do monkeydance\u0026quot;} } would render all other configuration items(such as environment variables, bind mounts and port mappings) void, if this branch was chosen. This is something I might have to account for at some time, by essentially generating the same values once per branch.\nBaby\u0026rsquo;s first macro The next part I wanted to tackle, was building the link for the WebUI, here I had to be creative, while the information needed was present, it is not easily accessible as it is stored in the format Groovy wants variables to be presented in a Jenkinsfile. The input would look like this for the SWAG image:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 repo_vars: - EXT_PIP = \u0026#39;certbot\u0026#39; - BUILD_VERSION_ARG = \u0026#39;CERTBOT_VERSION\u0026#39; - LS_USER = \u0026#39;linuxserver\u0026#39; - LS_REPO = \u0026#39;docker-swag\u0026#39; - CONTAINER_NAME = \u0026#39;swag\u0026#39; - DOCKERHUB_IMAGE = \u0026#39;linuxserver/swag\u0026#39; - DEV_DOCKERHUB_IMAGE = \u0026#39;lsiodev/swag\u0026#39; - PR_DOCKERHUB_IMAGE = \u0026#39;lspipepr/swag\u0026#39; - DIST_IMAGE = \u0026#39;alpine\u0026#39; - MULTIARCH=\u0026#39;true\u0026#39; - CI=\u0026#39;true\u0026#39; - CI_WEB=\u0026#39;false\u0026#39; - CI_PORT=\u0026#39;80\u0026#39; - CI_SSL=\u0026#39;false\u0026#39; - CI_DELAY=\u0026#39;30\u0026#39; - CI_DOCKERENV=\u0026#39;TEST_RUN=1\u0026#39; - CI_AUTH=\u0026#39;\u0026#39; - CI_WEBPATH=\u0026#39;\u0026#39; Getting the value of CI_PORT is not as easy as it should be, like using a getter on repo_vars does not work. Fortunately we can use some clever replacements and splits of each item under repo_vars to build a new and usable variable.\n1 2 3 4 5 6 {#- Create a real object from repo_vars -#} {%- set better_vars={} -%} {%- for i in repo_vars -%} {%- set i=(i | replace(\u0026#39; = \u0026#39;, \u0026#39;=\u0026#39;, 1) | replace(\u0026#39;=\u0026#39;, \u0026#39;¯\\_(ツ)_/¯\u0026#39;, 1) | replace(\u0026#34;\u0026#39;\u0026#34;, \u0026#34;\u0026#34;) | replace(\u0026#39;\u0026#34;\u0026#39;, \u0026#34;\u0026#34;)).split(\u0026#39;¯\\_(ツ)_/¯\u0026#39;) -%} {%- set x=(better_vars.__setitem__(i[0], i[1])) -%} {%- endfor -%} The new variable this creates is called better_vars, which is a dictionary-type. I and X is used as throwaway variables, as Jinja does not really have a good way to run straight up code(and with good reason, I imagine). Since repo_vars is a array-type, it serves as a iterator, and saves me from even more bodging. First order of business is to make the list uniform across both ways of placing the equals-sign, after this they don´t have any padding with spaces, I can start replacing and splitting the rest of the line until it has some resemblance of a typical python-like key-value pair.\nIn the first iteration of this code, there was no shrug, but a carefully chosen example above highlights how the split would work against us, the macro would fail when it arrived at CI_DOCKERENV, as the value of that key, is a key-value pair.\nWe are now working in Python land, so we can remove both double and single quotes, this can come back and bite us later, but as it stands right now this is not an issue. Currently CI_DOCKERENV='TEST_RUN=1' would be CI_DOCKERENV¯\\_(ツ)_/¯TEST_RUN=1, this is not useful yet, but we just need to convert this string to a key-value pair, easily done by using ¯\\_(ツ)_/¯ as the deliminator. Once we have this key-value pair we can use the __setitem__ function of the built in python dictionary-type.\nAfter all that there is now a variable that\u0026rsquo;s easier to work with, simply by using a getter.\n82 83 84 85 {# Set the WebUI link based on the link the CI runs against #} {% if better_vars.get(\u0026#34;CI_WEB\u0026#34;) and better_vars.get(\u0026#34;CI\u0026#34;) == \u0026#34;true\u0026#34; %} \u0026lt;WebUI\u0026gt;{{ \u0026#34;https\u0026#34; if better_vars.get(\u0026#34;CI_SSL\u0026#34;) == \u0026#34;true\u0026#34; else \u0026#34;http\u0026#34; }}://[IP]:[PORT:{{ better_vars.get(\u0026#34;CI_PORT\u0026#34;) }}]\u0026lt;/WebUI\u0026gt; {% endif %} This value is not supposed to hold a real URL, just the parts necessary for Unraid to build one. To do this it needs to know what container port the application is running on, we do this by using the syntax [PORT:80]. Now, if a user maps the container port of 80 to say host port 180, the Unraid webui button would now point to the ip of Unraid with port 180.\nMacros save the day When we told Squid(the guy running Community Applications) to switch us over to the new repo, we actually got blacklisted in CA because I forgot how more-than and the less-than sign got treated both by xml and CA(Community Applications) specifically. In CA they are blacklisted characters, simply having them in the user-facing parts of the template gets the whole template repository blacklisted. This prompted a new macro, one to filter out the illegal characters.\n29 30 31 {%- macro ca(str) -%} {{ str | replace(\u0026#34;\u0026lt;\u0026#34;, \u0026#34;\u0026#34;) | replace(\u0026#34;\u0026gt;\u0026#34;, \u0026#34;\u0026#34;) | replace(\u0026#34;[\u0026#34;, \u0026#34;\u0026#34;) | replace(\u0026#34;]\u0026#34;, \u0026#34;\u0026#34;) | replace(\u0026#34;\u0026amp;\u0026#34;, \u0026#34;and\u0026#34;) | escape }} {%- endmacro -%} This macro simply replaces \u0026lt;, \u0026gt;, [ and ] with nothing, while turning \u0026amp; to a word. For extra safety I put the escape filter at the end. At the time of writing there is no supported syntax in CA to make a hyperlink from a word. All \u0026ldquo;free text\u0026rdquo; input in the template goes trough this filter to prevent another blacklisting.\nGetting warm Now that I have gotten the taste, and gist of using macros, I made another couple of them to keep myself DRY(Don´t Repeat Yourself)\n32 33 34 35 36 37 38 {%- macro readme_date(str) -%} {%- set _date = (str | replace(\u0026#34;:\u0026#34;,\u0026#34;\u0026#34;)).split(\u0026#34;.\u0026#34;) -%} {{ \u0026#34;20\u0026#34; + _date[2] + \u0026#34;-\u0026#34; + _date[1] + \u0026#34;-\u0026#34; + _date[0] }} {%- endmacro -%} {%- macro mask(str) -%} {{ \u0026#34;true\u0026#34; if [\u0026#34;token\u0026#34;, \u0026#34;pass\u0026#34; ,\u0026#34;key\u0026#34;]|select(\u0026#34;in\u0026#34;, str|lower) else \u0026#34;false\u0026#34; }} {%- endmacro -%} Since the schema made for CA supports showing a changelog, we might as well use it, the metadata needed is already present in readme-vars.yml so no real work to get the data is needed. As this is the internet, and people come from different places, the dateformat we use is of course incompatible with the one CA accept, so I made a macro to convert mm.dd.yy to yyyy.mm.dd. Next up is a macro that gets called when creating environment variables, to determine if the variable should be masked.\nAlong with a entry to list potential requirements, the changelog macro is used like this:\n89 90 91 92 93 94 95 96 97 98 99 100 101 102 {% if unraid_requirement is defined and unraid_requirement != \u0026#34;\u0026#34; %} \u0026lt;Requires\u0026gt;{{ unraid_requirement }}\u0026lt;/Requires\u0026gt; {% endif %} {# Create changelog #} {% if changelogs is defined and changelogs %} \u0026lt;Date\u0026gt;{{ readme_date(changelogs |map(attribute=\u0026#39;date\u0026#39;) | first) }}\u0026lt;/Date\u0026gt; \u0026lt;Changes\u0026gt; {% for item in changelogs %} ### {{ readme_date( item.date ) }} - {{ ca(item.desc) }} {% endfor %} \u0026lt;/Changes\u0026gt; {% endif %} The long boi Another thing you might want to do with your container is passing along some less common parameters, like memory or cpu limits. This is something I had to tackle with \u0026ldquo;\u0026ldquo;code\u0026rdquo;\u0026rdquo;. As the metadata for this is more literal to the real compose way of writing it, implementing support for security options is also coming.\n8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 {#- Create ExtraParam for REQUIRED stuff-#} {%- set ExtraParam=[] -%} {%- set x=ExtraParam.append(\u0026#34;--hostname=\u0026#34; + param_hostname) if param_usage_include_hostname is sameas true -%} {%- set x=ExtraParam.append(\u0026#34;--mac-address=\u0026#34; + param_mac_address) if param_usage_include_mac_address is sameas true -%} {%- if cap_add_param is defined -%} {%- for item in cap_add_param_vars -%} {%- set x=ExtraParam.append(\u0026#34;--cap-add=\u0026#34; + item.cap_add_var) -%} {%- endfor -%} {#- custom_params -#} {%- if custom_params is defined -%} {%- for item in custom_params -%} {%- if item.array is not defined -%} {%- set x=ExtraParam.append(\u0026#34;--\u0026#34; + item.name+ \u0026#34;=\u0026#34; + item.value) -%} {%- else -%} {%- for item2 in item.value -%} {%- set x=ExtraParam.append(\u0026#34;--\u0026#34; + item.name+ \u0026#34;=\u0026#34; + item2) -%} {%- endfor -%} {%- endif -%} {%- endfor -%} {%- endif -%} {%- endif -%} This logic defines a variable called ExtraParam, then massages different entries from the metadata, to a array of strings, where each item in the array is a valid docker run argument.\nThe normal stuff This article is not written in a chronological order based on the development cycle, rather following the structure in the end product. You can see this by the lack of macros in the rest of the template, if I ever have to do major revisions of this template, turning this into macros would be the first thing to do.\nPorts A good chunk of the applications we bundle, uses multiple ports for different purposes, this is why we have sections in our metadata for optional ports. Thankfully Unraid has the ability to display if a port is optional or not. The schema also exposes the protocol part of a port mapping, a value we also have support for in our metadata. Now we you can see the CA macro in action, it is used to clean characters from our metadata.\nThere is a lot of logic present to build the description and name of these ports. It will automatically name the first port as \u0026ldquo;WebUI\u0026rdquo;, or it will fall back to the naming Unraid would use. For the description it will use the one defined in the metadata, or fall back to the value Unraid would have used. Mostly the same logic is used in the optional ports.\n103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 {# Set required ports, gets the name from the name atribute if present, or \u0026#34;WebUI\u0026#34; if it is the first port #} {% if param_usage_include_ports | default(false) %} {% for item in param_ports %} {% set port, proto=item.internal_port.split(\u0026#39;/\u0026#39;) if \u0026#34;/\u0026#34; in item.internal_port else [item.internal_port, false] %} {#- Logic to get the protocol #} \u0026lt;Config Name=\u0026#34;{{ ca(item.name) if item.name is defined else \u0026#34;WebUI\u0026#34; if loop.first else \u0026#34;Port: \u0026#34; + port }}\u0026#34; Target=\u0026#34;{{ port }}\u0026#34; Default=\u0026#34;{{ ca(item.external_port) }}\u0026#34; Mode=\u0026#34;{{ proto if proto else \u0026#34;tcp\u0026#34; }}\u0026#34; Description=\u0026#34;{{ ca(item.port_desc) if item.port_desc is defined else \u0026#34;Container Port: \u0026#34; + port }}\u0026#34; Type=\u0026#34;Port\u0026#34; Display=\u0026#34;always\u0026#34; Required=\u0026#34;true\u0026#34; Mask=\u0026#34;false\u0026#34;/\u0026gt; {% endfor %} {% endif %} {#- Set required ports, gets the name from the name atribute if present, or \u0026#34;WebUI\u0026#34; if it is the first port #} {#- Set optional ports #} {% if opt_param_usage_include_ports | default(false) %} {% for item in opt_param_ports %} {% set port, proto=item.internal_port.split(\u0026#39;/\u0026#39;) if \u0026#34;/\u0026#34; in item.internal_port else [item.internal_port, false] %} {#- Logic to get the protocol #} \u0026lt;Config Name=\u0026#34;{{ ca(item.name) if item.name is defined else \u0026#34;Port: \u0026#34; + port }}\u0026#34; Target=\u0026#34;{{ port }}\u0026#34; Default=\u0026#34;{{ ca(item.external_port) }}\u0026#34; Mode=\u0026#34;{{ proto if proto else \u0026#34;tcp\u0026#34; }}\u0026#34; Description=\u0026#34;{{ ca(item.port_desc) if item.port_desc is defined else \u0026#34;Container Port: \u0026#34; + port }}\u0026#34; Type=\u0026#34;Port\u0026#34; Display=\u0026#34;always\u0026#34; Required=\u0026#34;false\u0026#34; Mask=\u0026#34;false\u0026#34;/\u0026gt; {% endfor %} {% endif %} {#- Set optional ports #} Volumes The logic used for volumes is pretty much a copy-paste from the ports-logic, but instead of looking \u0026ldquo;WebUI\u0026rdquo;, it is trying to find a volume to call \u0026ldquo;Appdata\u0026rdquo;. There is also a piece of extra logic to see if a bind-volume is marked as Read Only.\n126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 {#- Set required volumes, gets the name from the name atribute if present, or \u0026#34;Appdata\u0026#34; if it is the /config location #} {% if param_usage_include_vols | default(false) %} {% for item in param_volumes %} {% set path, mode=item.vol_path.split(\u0026#39;:\u0026#39;) if \u0026#34;:\u0026#34; in item.vol_path else [item.vol_path, false] %} {#- Logic to get the mode #} \u0026lt;Config Name=\u0026#34;{{ ca(item.name) if item.name is defined else \u0026#34;Appdata\u0026#34; if path == \u0026#34;/config\u0026#34; else \u0026#34;Path: \u0026#34; + path }}\u0026#34; Target=\u0026#34;{{ ca(path) }}\u0026#34; Default=\u0026#34;{{ ca(item.vol_host_path) if item.default is defined and item.default is sameas true }}\u0026#34; Mode=\u0026#34;{{ mode if mode else \u0026#34;rw\u0026#34; }}\u0026#34; Description=\u0026#34;{{ ca(item.desc) if item.desc is defined else \u0026#34;Path: \u0026#34; + path }}\u0026#34; Type=\u0026#34;Path\u0026#34; Display=\u0026#34;{{ \u0026#34;advanced\u0026#34; if path == \u0026#34;/config\u0026#34; else \u0026#34;always\u0026#34; }}\u0026#34; Required=\u0026#34;true\u0026#34; Mask=\u0026#34;false\u0026#34;/\u0026gt; {% endfor %} {% endif %} {#- Set required volumes, gets the name from the name atribute if present, or \u0026#34;Appdata\u0026#34; if it is the /config location #} {#- Set optional volumes #} {% if opt_param_usage_include_vols | default(false) %} {% for item in opt_param_volumes %} {% set path, mode=item.vol_path.split(\u0026#39;:\u0026#39;) if \u0026#34;:\u0026#34; in item.vol_path else [item.vol_path, false] %} {#- Logic to get the mode #} \u0026lt;Config Name=\u0026#34;{{ ca(item.name) if item.name is defined else \u0026#34;Appdata\u0026#34; if path == \u0026#34;/config\u0026#34; else \u0026#34;Path: \u0026#34; + path }}\u0026#34; Target=\u0026#34;{{ ca(path) }}\u0026#34; Default=\u0026#34;{{ ca(item.vol_host_path) if item.default is defined and item.default is sameas true }}\u0026#34; Mode=\u0026#34;{{ mode if mode else \u0026#34;rw\u0026#34; }}\u0026#34; Description=\u0026#34;{{ ca(item.desc) if item.desc is defined else \u0026#34;Path: \u0026#34; + path }}\u0026#34; Type=\u0026#34;Path\u0026#34; Display=\u0026#34;always\u0026#34; Required=\u0026#34;false\u0026#34; Mask=\u0026#34;false\u0026#34;/\u0026gt; {% endfor %} {% endif %} {#- Set optional volumes #} Variables The base of the logic for variables is also based on the ports-logic, but it does filter away some variables we hardcode, or variables that Unraid automatically manages.\nThe id´s for puid and guid in Unraid, is following a agreed upon id from the early days, the 99 user is ´nobody´.\n135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 {% set skip_envs=[\u0026#34;puid\u0026#34;, \u0026#34;pgid\u0026#34;, \u0026#34;tz\u0026#34;, \u0026#34;umask\u0026#34;] %} {#- Drop envs that are either hardcoded, or automaticcly added by unraid #} {#- Set required variables, gets the name from the name atribute #} {% if param_usage_include_env | default(false) %} {% for item in param_env_vars if not item.env_var | lower is in skip_envs %} \u0026lt;Config Name=\u0026#34;{{ ca(item.name) if item.name is defined else item.env_var }}\u0026#34; Target=\u0026#34;{{ item.env_var }}\u0026#34; Default=\u0026#34;{{ item.env_options | join(\u0026#39;|\u0026#39;) if item.env_options is defined else ca(item.env_value) }}\u0026#34; Description=\u0026#34;{{ ca(item.desc) if item.desc is defined else \u0026#34;Variable: \u0026#34; + path }}\u0026#34; Type=\u0026#34;Variable\u0026#34; Display=\u0026#34;always\u0026#34; Required=\u0026#34;true\u0026#34; Mask=\u0026#34;{{ mask(item.env_var) }}\u0026#34;/\u0026gt; {% endfor %} {% endif %} {#- Set required variables, gets the name from the name atribute #} {#- Set optional variables #} {% if opt_param_usage_include_env | default(false) %} {% for item in opt_param_env_vars if not item.env_var | lower is in skip_envs %} \u0026lt;Config Name=\u0026#34;{{ ca(item.name) if item.name is defined else item.env_var }}\u0026#34; Target=\u0026#34;{{ item.env_var }}\u0026#34; Default=\u0026#34;{{ ca(item.env_value) }}\u0026#34; Description=\u0026#34;{{ ca(item.desc) if item.desc is defined else \u0026#34;Variable: \u0026#34; + path }}\u0026#34; Type=\u0026#34;Variable\u0026#34; Display=\u0026#34;always\u0026#34; Required=\u0026#34;false\u0026#34; Mask=\u0026#34;{{ mask(item.env_var) }}\u0026#34;/\u0026gt; {% endfor %} {% endif %} {#- Set optional variables #} \u0026lt;Config Name=\u0026#34;PUID\u0026#34; Target=\u0026#34;PUID\u0026#34; Default=\u0026#34;99\u0026#34; Description=\u0026#34;Container Variable: PUID\u0026#34; Type=\u0026#34;Variable\u0026#34; Display=\u0026#34;advanced\u0026#34; Required=\u0026#34;true\u0026#34; Mask=\u0026#34;false\u0026#34;/\u0026gt; \u0026lt;Config Name=\u0026#34;PGID\u0026#34; Target=\u0026#34;PGID\u0026#34; Default=\u0026#34;100\u0026#34; Description=\u0026#34;Container Variable: PGID\u0026#34; Type=\u0026#34;Variable\u0026#34; Display=\u0026#34;advanced\u0026#34; Required=\u0026#34;true\u0026#34; Mask=\u0026#34;false\u0026#34;/\u0026gt; \u0026lt;Config Name=\u0026#34;UMASK\u0026#34; Target=\u0026#34;UMASK\u0026#34; Default=\u0026#34;022\u0026#34; Description=\u0026#34;Container Variable: UMASK\u0026#34; Type=\u0026#34;Variable\u0026#34; Display=\u0026#34;advanced\u0026#34; Required=\u0026#34;false\u0026#34; Mask=\u0026#34;false\u0026#34;/\u0026gt; Devices The logic for devices is also very similar, without any special treatment.\n153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 {# Set required devices, gets the name from the name atribute #} {% if param_device_map | default(false) %} {% for item in param_devices %} \u0026lt;Config Name=\u0026#34;{{ ca(item.name) if item.name is defined else item.device_path }}\u0026#34; Default=\u0026#34;{{ item.device_path }}\u0026#34; Description=\u0026#34;{{ ca(item.desc) if item.desc is defined else \u0026#34;Device: \u0026#34; + path }}\u0026#34; Type=\u0026#34;Device\u0026#34; Display=\u0026#34;always\u0026#34; Required=\u0026#34;true\u0026#34; Mask=\u0026#34;false\u0026#34;/\u0026gt; {% endfor %} {% endif %} {#- Set required variables, gets the name from the name atribute #} {#- Set optional devices #} {% if opt_param_device_map | default(false) %} {% for item in opt_param_devices %} \u0026lt;Config Name=\u0026#34;{{ ca(item.name) if item.name is defined else item.device_path }}\u0026#34; Default=\u0026#34;{{ item.device_path }}\u0026#34; Description=\u0026#34;{{ ca(item.desc) if item.desc is defined else \u0026#34;Device: \u0026#34; + path }}\u0026#34; Type=\u0026#34;Device\u0026#34; Display=\u0026#34;always\u0026#34; Required=\u0026#34;false\u0026#34; Mask=\u0026#34;false\u0026#34;/\u0026gt; {% endfor %} {% endif %} {#- Set optional devices #} \u0026lt;/Container\u0026gt; Finishing up The last step is to tie this template into the Ansible play, which I won´t cover here, as it´s not a bodge, like this template is.\n","permalink":"https://nonsense.fyi/posts/templating-templates-with-templates/","summary":"\u003cp\u003eOver at Linuxserver we use Ansible for it\u0026rsquo;s convenient integration with Jinja, a templating engine for Python, along with its powerful framework to execute shell commands. With this we are able to automate quite a lot of work, like generating a uniform and recognizable README, or injecting files into the container(don\u0026rsquo;t worry, the files are committed and pushed to GitHub before the image is built) for some logic. In order for all this to work, most of the metadata is filled into two YAML files in each repository, \u003ccode\u003ejenkins-vars.yml\u003c/code\u003e and \u003ccode\u003ereadme-vars.yml\u003c/code\u003e which are then presented to Ansible as variables used for consumption in the templates.\u003c/p\u003e","title":"Templating templates with templates"},{"content":"Now that I have set up this server as I want, the next step is to ensure it stays this way. One of the things I do to achieve this, is by setting up monitoring, and yes, it is very overkill for this setup. I went with a set of tools that I know, with some new additions which has been on my list of things to look into for a while, mainly Loki and Promtail. They are both projects from Grafana which are heavily focused on logs, both presenting them and parse them.\nI\u0026rsquo;m basing this on the TIG(Telegraf, InfluxDB, Grafana) stack, because I didn\u0026rsquo;t want to spend time learning Prometheus, however implementing this took more time than it should\u0026rsquo;ve, Loki even got updated while setting it up. As I said earlier, I wanted to add some new tools in my \u0026ldquo;normal\u0026rdquo; stack. Just because Telegraf can do logs, does not mean it should. It can handle both syslog with a syslog-listener plugin, and docker-logs with the use of the docker socket, but Grafana has no good way of presenting them (I don\u0026rsquo;t blame them).\nSetting up InfluxDB Setting up InfluxDB in docker is quite easy. This is all it takes.\n1 2 3 4 5 6 7 8 9 10 influxdb: image: influxdb:latest container_name: influxdb environment: - TZ=${TZ} ports: - 127.0.0.1:8086:8086 volumes: - /opt/appdata/influxdb/:/var/lib/influxdb restart: unless-stopped Setting up Telegraf Again, I choose to have a service running on the host, rather than in a container. I did this because Fail2Ban run on the host, having a container talking to a service on the host when the Telegraf plugin uses the command-line to interface with the service sounded like an unnecessary challenge.\nMy telegraf.conf is relatively small, only because I added the sections I were going to use, rather than using the one included which has all the options in it. Depending on your experience with Telegraf, it might look a bit complicated, especially the outputs. It is in fact not that advanced.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 [[outputs.influxdb]] database = \u0026#34;telegraf\u0026#34; # required retention_policy = \u0026#34;\u0026#34; write_consistency = \u0026#34;any\u0026#34; timeout = \u0026#34;5s\u0026#34; [outputs.influxdb.tagdrop] influx_database = [\u0026#34;docker\u0026#34;, \u0026#34;nginx\u0026#34;, \u0026#34;system\u0026#34;, \u0026#34;web\u0026#34;] [[outputs.influxdb]] database = \u0026#34;system\u0026#34; # required retention_policy = \u0026#34;\u0026#34; write_consistency = \u0026#34;any\u0026#34; timeout = \u0026#34;5s\u0026#34; tagexclude = [\u0026#34;influx_database\u0026#34;] [outputs.influxdb.tagpass] influx_database = [\u0026#34;system\u0026#34;] This is mainly doing two things, to achieve the goal of diverting each type of metric into their own database in InfluxDB. Having them all in the same one isn\u0026rsquo;t really a problem; it just makes it easier for me. To do this Telegraf has to add a tag to the metric it collects.\n1 2 3 4 5 6 7 [[inputs.cpu]] percpu = true totalcpu = true collect_cpu_time = false report_active = false [inputs.cpu.tags] influx_database = \u0026#34;system\u0026#34; Here I tell Telegraf to tag all the metrics from the cpu input with the tag influx_database set to system. Telegraf can use the tagpass and tagdrop to further filter the metrics. We use tagpass to tell Telegraf which tags this output should allow. In the example I tell it to allow all metrics tagged with influx_database set to system. The tagdrop does the opposite, it tells Telegraf which tags it should not allow. In order to keep the output somewhat clean, I tell Telegraf to not pass the influx_database tag to InfluxDB.\nOther than configuring nginx to show nginx_stub status on 127.0.0.1:8080, the rest of the telegraf.conf is pretty much stock.\n1 2 3 4 5 6 7 server { listen 127.0.0.1:8080; location / { stub_status on; access_log off; } } Setting up Grafana I noticed a while back, that you can configure Grafana purely by using enviroment variables, this is awesome because it can also be set up to not use the embedded sqlite database. Which means I don\u0026rsquo;t need to give it a volume mount for persistent data, I did however give it a mount for using the socket instead of using http between nginx and Grafana.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 grafana: image: grafana/grafana:latest container_name: grafana hostname: ${HOSTNAME} user: \u0026#34;1000:33\u0026#34; environment: - TZ=${TZ} - GF_AUTH_ANONYMOUS_ENABLED=true - GF_AUTH_ANONYMOUS_ORG_NAME=Nonsense - GF_AUTH_ANONYMOUS_ORG_ROLE=Viewer - GF_DATABASE_URL=mysql://grafana:${GrafanaDB_PASS}@mariadb/grafana - GF_INSTALL_PLUGINS=grafana-clock-panel,grafana-worldmap-panel,grafana-piechart-panel - GF_SECURITY_ADMIN_USER=Roxedus - GF_SECURITY_ALLOW_EMBEDDING=true - GF_SECURITY_COOKIE_SECURE=true - GF_SERVER_PROTOCOL=socket - GF_SERVER_SOCKET=/opt/socket/grafana.socket # - GF_SECURITY_ADMIN_PASSWORD=${SQL_ROOT} volumes: - /opt/socket/:/opt/socket I renamed the default organization in Grafana, which means that the default anonymous(guest) access break. I fix that by declaring which organization and role the anonymous user is attached to. Then I tell Grafana how to connect to the database, and what plugins it should install on start. Since I can, I also set the admin username and password, instead of updating the user after the fact.\nUnlike Ghost, Grafana doesnt have a way to set permissions on the socket. Therefore, Grafana runs as my user, as part of the www-data group. It is dirty, but it works.\nReading a simple logfile with Promtail The next part was getting Loki and Promtail set up. Oh boy what a task this was. I don\u0026rsquo;t know why, but there isn\u0026rsquo;t much content about this out there, if I were to guess people run the whole ELK stack. Which is way overkill for what I want to achieve here. The biggest hurdle personally was the fact that it was written in go, and therefore the config used some go-specific rules. Did you know that go\u0026rsquo;s implementation of regex is very minimal? Or that the time-formatting is done without the notion of deliminators like YYYY-MM-DD? I for sure did not.\nGetting the containers running was pretty simple, once you have downloaded the basic configuration files for Loki and Promtail. I choice to not use the default location inside the containers, I followed the practice of having all persistent data inn /config since I can override the config.file location with the command feature of docker.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 loki: image: grafana/loki:${LOKI_VER} hostname: ${HOSTNAME} container_name: loki environment: - TZ=${TZ} volumes: - /opt/appdata/loki/config:/config:ro command: -config.file=/config/loki-config.yaml promtail: image: grafana/promtail:${LOKI_VER} container_name: promtail environment: - TZ=${TZ} depends_on: - loki volumes: - /opt/appdata/promtail:/config:ro - /opt/logs:/opt/logs:ro - /var/log:/var/log:ro - /etc/machine-id:/etc/machine-id:ro command: -config.file=/config/promtail-config.yaml My Loki service is not too far off from the example Loki provides. I added the hostname for the machine and set the TZ. I will come back to the logging part later.\nFor Promtail I gave it two volume-mounts for logs, where /opt/logs is where the logs from the applications I run with Docker are located (not to be confused with docker logs). The next thing I did was to change the promtail-config.yaml file to tell Promtail to read the logs in /opt.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 server: http_listen_port: 9080 grpc_listen_port: 0 positions: filename: /tmp/positions.yaml clients: - url: http://loki:3100/loki/api/v1/push scrape_configs: - job_name: system pipeline_stages: static_configs: - targets: - localhost labels: job: varlogs __path__: /var/log/**/**log - targets: - localhost labels: job: containers __path__: /opt/logs/**/*log I could have added the __path__ to the varlogs job, but I added it as a new job to be able to filter it out easily in Grafana. They would present themselves something like this:\nAs well as labeling the data with the job-name, it also labels it with the filename, which is neat when you want to focus on a specific logfile. If you look closely you can see that Loki didn\u0026rsquo;t read the timestamp properly. This is something we can fix manually and depending on the original time-format its not that difficult, after getting over the hurdles that is go\u0026rsquo;s time-formatting. The documentation does cover it, but it takes a few clicks to get there, so here is a link. There is a catch here though, especially if you know language like python. go\u0026rsquo;s time-formater doesn\u0026rsquo;t handle milliseconds with commas, there is an issue open about this from 2013.\n2021 Note: Looks like this is no longer the case\nYou can still get the timestamp, but there is an additional step that need to be taken for that. I ran into that issue with the fail2ban logs, and I will walk you trough how I solved it. Here is a line from that log.\n2020-05-27 21:51:50,138 fail2ban.filter [9474]: INFO [sshd] Found 1.1.1.1 - 2020-05-27 21:51:50 The first thing that had to be done was extracting the timestamp from the line. This is done with a regex stage. Note the that the backslashes needs to be escaped.\n1 2 3 4 - regex: expression: \u0026#34;(?P\u0026lt;time\u0026gt;\\\\d{4}-\\\\d{2}-\\\\d{2} \\\\d{2}:\\\\d{2}:\\\\d{2},\\\\d{0,3}) (?P\u0026lt;message\u0026gt;fail2ban.*(?P\u0026lt;pid\u0026gt;\\\\[\\\\d*\\\\]: )(?P\u0026lt;level\u0026gt;[A-Z]{4,7}) .*)\u0026#34; I am also going to be extracting a few other things, like the message itself, the PID and the log-level, to use in other stages. You can look at the regex in action on regex101.com.\nNow I can use the content of time group as a variable in Promtail. The next step is to replace the comma with a dot, so Promtail can understand the time-format. This is done with a template step. That is another thing worth looking into, go\u0026rsquo;s template system. After reading about the replace function on the string function, it was pretty clear that it was the last piece of the puzzle I needed to solve before putting it all together.\n1 2 3 - template: source: time template: \u0026#39;{{ Replace .Value \u0026#34;,\u0026#34; \u0026#34;.\u0026#34; -1 }}\u0026#39; Now we have the time variable available, because all the capture groups created with the regex step is exported for us to use. I pass it to the template. .Value is the value from the variable set as source. Then it replaces , with . for -1 times, -1 is the same as replace all. The output of this step is a new time variable, same name, new content. We can now define the timestamp in the next stage.\n1 2 3 - timestamp: source: time format: \u0026#34;2006-01-02 15:04:05.000\u0026#34; This is how you define a time-format in GO. This is it. It is a clever way to do it, it uses a set value for each component. The year it looks for is 2006, so it\u0026rsquo;s either 06 or 2006. The month has a few more options 1, 01, Jan or January. So, in a sense it is all pretty easy to read. All the possibilities for the components are in the documentation for the timestamp stage.\nNow Promtail know what time the log was sent for.\nI am not happy with the result; it has data I don\u0026rsquo;t need, there is more I can do. If you look at the initial line from the log, it also has the time at the end, neither do I need the PID. I can fix this with another template step and a regex step.\n1 2 3 - template: source: message template: \u0026#39;{{ Replace .Value .pid \u0026#34;\u0026#34; -1 }}\u0026#39; In this stage it reads the message extracted from the initial regex, then it replaces the content of the capture-group/label called pid with nothing.\n1 2 3 - regex: expression: \u0026#39;(?P\u0026lt;message\u0026gt;.*)(?: - \\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\u0026#39; source: message This regex step uses the message passed from the previous template step and creates a new message capture-group, the important thing here is that it places the eventual timestamp in its own capture group, which is not a part of the message group.\nIf I were to start Promtail now, it would still see the line as it is in the log. Promtail needs to be told what it should be passing along, so we use the output stage.\n1 2 - output: source: message However it also needs to be told what to apply all these steps to, so a final configuration to get Promtail to read and parse the Fail2Ban log would look something like this.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 server: http_listen_port: 9080 grpc_listen_port: 0 positions: filename: /tmp/positions.yaml clients: - url: http://loki:3100/loki/api/v1/push scrape_configs: - job_name: system pipeline_stages: - match: selector: \u0026#39;{filename=~\u0026#34;.*fail2ban.log\u0026#34;}\u0026#39; stages: - regex: expression: \u0026#34;(?P\u0026lt;time\u0026gt;\\\\d{4}-\\\\d{2}-\\\\d{2} \\\\d{2}:\\\\d{2}:\\\\d{2},\\\\d{0,3})?\\\\D? (?P\u0026lt;message\u0026gt;fail2ban.*(?P\u0026lt;pid\u0026gt;\\\\[\\\\d*\\\\]: )(?P\u0026lt;level\u0026gt;[A-Z]{4,7}) .* (?:(?:\\\\[|Jail \u0026#39;)(?P\u0026lt;jail\u0026gt;\\\\D*)(?:\\\\]|\u0026#39;))?.*)\u0026#34; - template: source: message template: \u0026#39;{{ Replace .Value .pid \u0026#34;\u0026#34; -1 }}\u0026#39; - regex: expression: \u0026#39;(?P\u0026lt;message\u0026gt;.*)(?: - \\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\u0026#39; source: message - template: source: time template: \u0026#39;{{ Replace .Value \u0026#34;,\u0026#34; \u0026#34;.\u0026#34; -1 }}\u0026#39; - timestamp: source: time format: \u0026#34;2006-01-02 15:04:05.000\u0026#34; - output: source: message static_configs: - targets: - localhost labels: job: varlogs __path__: /var/log/**/**log - targets: - localhost labels: job: containers __path__: /opt/logs/**/*log In order for Promtail to know where to apply these stages, it needs to match it to something. Grafana is then again presenting a new concept with Loki, LogQL. It is an adaptation of Prometheus\u0026rsquo; query language, focused on logs. This is used in the selector, I kept mine simple. It is only matching against filenames ending with fail2ban.log, the filename includes paths, which is why there is a wildcard in front.\nReading docker-logs with Promtail Loki has a docker log-driver, which as the time of writing has a few deal breaking issues, mainly #2017. To work around this, I found that sending docker-logs to the journal works for my use case. The Promtail config for this was surprisingly easy, by just setting up the journal job example from Promtails documentation.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 server: http_listen_port: 9080 grpc_listen_port: 0 positions: filename: /tmp/positions.yaml clients: - url: http://loki:3100/loki/api/v1/push scrape_configs: - job_name: system pipeline_stages: - job_name: journal journal: max_age: 12h labels: job: systemd-journal relabel_configs: - source_labels: [\u0026#34;__journal__systemd_unit\u0026#34;] target_label: \u0026#34;unit\u0026#34; That will give us the journal from the host since /var/log/journal is mapped.\nThere is a few ways to tell docker to log to the journal, however I am going to do it in my docker-compose. I simply just add a few lines to the containers I want to see in Grafana.\n1 2 3 4 logging: driver: journald options: mode: non-blocking My Matomo instance would then look something like this.\n1 2 3 4 5 6 7 8 9 10 11 12 matomo: image: matomo:fpm-alpine container_name: matomo logging: driver: journald options: mode: non-blocking depends_on: - mariadb volumes: - /opt/appdata/matomo:/var/www/html - /opt/logs/matomo:/logz Now I can see the logs from the Matomo container in Grafana, however it does not have a label I can use to sort out all the entries which does not come from the container. I can do that by adding a relabel action in the job-definition. Promtail presents all metadata from the journal with the __journal_ label-prefix. This means that we can access the CONTAINER_NAME attribute docker adds to the logfile with the label __journal_container_name, so by adding a relabel_config we can get the container name as a label in Promtail.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 server: http_listen_port: 9080 grpc_listen_port: 0 positions: filename: /tmp/positions.yaml clients: - url: http://loki:3100/loki/api/v1/push scrape_configs: - job_name: system pipeline_stages: - job_name: journal journal: max_age: 12h labels: job: systemd-journal relabel_configs: - source_labels: [\u0026#34;__journal__systemd_unit\u0026#34;] target_label: \u0026#34;unit\u0026#34; - source_labels: [\u0026#34;__journal_container_name\u0026#34;] target_label: \u0026#34;container\u0026#34; It will present the logs similar to this:\n","permalink":"https://nonsense.fyi/posts/deploying-loki-and-promtail-together-with-the-tig-stack/","summary":"\u003cp\u003eNow that I have set up this server as I want, the next step is to ensure it stays this way. One of the things I do to achieve this, is by setting up monitoring, and yes, it is very overkill for this setup. I went with a set of tools that I know, with some new additions which has been on my list of things to look into for a while, mainly \u003ca href=\"https://github.com/grafana/loki/tree/master\" target=\"_blank\" rel=\"noopener\"\u003eLoki\u003c/a\u003e and Promtail. They are both projects from \u003ca href=\"https://github.com/grafana\" target=\"_blank\" rel=\"noopener\"\u003eGrafana\u003c/a\u003e which are heavily focused on logs, both presenting them and parse them.\u003c/p\u003e","title":"Deploying Loki and Promtail Together With the TIG Stack"}]