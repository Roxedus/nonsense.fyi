<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Road to CKAD | nonsense.fyi</title>
<meta name=keywords content="Kubernetes,Homelab,Certification"><meta name=description content="My adventure in Certifying myself in Kubernetes"><meta name=author content="Simen Røstvik"><link rel=canonical href=https://nonsense.fyi/posts/road-to-ckad/><link crossorigin=anonymous href=/assets/css/stylesheet.3c0edd7bac3381aa7fafecd3dd6f74529e2df70d42ea92abe524e477df9c174b.css integrity="sha256-PA7de6wzgap/r+zT3W90Up4t9w1C6pKr5STkd9+cF0s=" rel="preload stylesheet" as=style><link rel=icon href=https://nonsense.fyi/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://nonsense.fyi/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://nonsense.fyi/favicon-32x32.png><link rel=apple-touch-icon href=https://nonsense.fyi/apple-touch-icon.png><link rel=mask-icon href=https://nonsense.fyi/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async defer data-domain=nonsense.fyi src=https://hosted.roxedus.net/js/script.outbound-links.js></script><script>getCurrentVisitors().then(showCurrentVisitors),setInterval(()=>getCurrentVisitors().then(showCurrentVisitors),1e4);function showCurrentVisitors(e=0){const t=document.getElementById("current-visitors-container");if(!t)return console.info("no container to show current visitors found");t.innerHTML=`
    <a aria-hidden tabindex="-1" href="https://views.roxedus.net/nonsense.fyi" rel="nofollow noopener external" target="_blank">
        ${e}&nbsp;<svg width="1em" height="1em" viewBox="0 0 16 16" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" d="M10 5a2 2 0 1 1-4 0 2 2 0 0 1 4 0zM8 8a3 3 0 1 0 0-6 3 3 0 0 0 0 6zm6 5c0 1-1 1-1 1H3s-1 0-1-1 1-4 6-4 6 3 6 4zm-1-.004c-.001-.246-.154-.986-.832-1.664C11.516 10.68 10.289 10 8 10c-2.29 0-3.516.68-4.168 1.332-.678.678-.83 1.418-.832 1.664h10z"/></svg>
    </a>
    `.trim()}function getCurrentVisitors(){return window.fetch("https://current-visitors.nonsense.fyi/").then(e=>e.text())}</script><meta property="og:title" content="Road to CKAD"><meta property="og:description" content="My adventure in Certifying myself in Kubernetes"><meta property="og:type" content="article"><meta property="og:url" content="https://nonsense.fyi/posts/road-to-ckad/"><meta property="og:image" content="https://nonsense.fyi/images/captain.jpg"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-10-13T00:00:00+02:00"><meta property="article:modified_time" content="2023-10-13T00:00:00+02:00"><meta property="og:site_name" content="Nonsense, For Your Information"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://nonsense.fyi/images/captain.jpg"><meta name=twitter:title content="Road to CKAD"><meta name=twitter:description content="My adventure in Certifying myself in Kubernetes"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://nonsense.fyi/posts/"},{"@type":"ListItem","position":2,"name":"Road to CKAD","item":"https://nonsense.fyi/posts/road-to-ckad/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Road to CKAD","name":"Road to CKAD","description":"My adventure in Certifying myself in Kubernetes","keywords":["Kubernetes","Homelab","Certification"],"articleBody":"So I took some certs Ive recently gone trough the Kubernetes Administrator, Developer and Security Specialist certifications. In typical fashion I broke some stuff on my way to get there.\nThe git references in this article are not up-to-date, as later deployments has made me consolidating multiple repos to a single infra repo. Preparation This is everything I used to prepare for these certifications:\nTime Some smol computing boys, three Raspberry PI 4 8GB was hurt for this blogpost Lectures with practical tests, I went with the courses from KodeKloud A not so big LXC running on Proxmox 7 Patience Time Proxmox and LXC For some unrelated infrastructure, I set up Proxmox. I attempted to put most of the configuration in Ansible, but its just not feasible in the long run. The Proxmox host is however not fully ClickOps based, as packages, swap and the certificate(fetched from OpnSense) is managed by the playbook.\nControlplane LXC Because sourcing additional Raspberry PI’s was an impossible task (and still isn’t easy), I thought “Hey, my Proxmox host is only doing Home-Assistant right know, I can squeeze more onto it”.\nSince I am who I am, I decided to dabble in something new, enter Linux Containers(lxc)1.\nThe reason for researching this method is straightforward; VMs are heavy, OCI2 containers are light. I needed something in between. This host is limited on resources, but not starved for them. Therefore avoiding running another kernel and subsystem would be preferred, this ruled out Virtual Machines. Another option I am very comfortable with, is Docker. However, (stock) Kubernetes really wants a fully-fledged init system running and I am not crazy enough to run a complete Systemd instance in a Docker container.\nAs it took multiple attempts to get the LXC configured to make kubeadm happy, I saved that file once all the issues was ironed out. It looks something like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 arch: amd64 cores: 2 hostname: controlplane memory: 4096 net0: name=eth0,bridge=vmbr0,firewall=1,gw=,hwaddr=,ip=,type=veth ostype: ubuntu rootfs: ,size=50G searchdomain: kube. swap: 0 features: fuse=1,mount=nfs,nesting=1 lxc.apparmor.profile: unconfined lxc.cap.drop: lxc.cgroup.devices.allow: a lxc.mount.auto: proc:rw sys:rw I later tried to define this LXC with Terraform, but ran mostly into auth issues. This LXC needed to be privileged, which means it cannot be created by anyone else then the “root” user, making a service account useless. I have also opted for using MFA on the root account, which made user/password moot.\nPlaying with the playbook As mentioned, I have a Ansible playbook for infrastructure running inside my lan. This playbook gives me a familiar and uniform environment for all hosts. It handles common packages I have come to expect being present on the machine im working on, while setting up my user with a public key, and the prompt in my shell.\nTo get my environment ready, I wrote a Kubernetes role into my existing Ansible Infra playbook, this takes care of both Kubeadm’s and Kubernetes’ requirements. This includes disabling swap, setting up containerd as a runtime(thanks Jeff) and a whole lot of kernel tuning. Using some roles in the Ansible Galaxy might have saved me some time here, but before I landed on using Jeff’s containerd role, I tried to use some of the cri-o roles, but I spent too much time digging in apt repos to mitigate the fact that Kubernetes with ARM64 nodes is less adopted than I thought(more on this later). I quickly abandoned the idea of using a Galaxy role for cri-o, after struggling to get cri-o going at all, I gave up on cri-o altogether and settled for the containerd role.\nSince my lab is run on somewhat unconventional setups, I had to make several changes for Raspberry PIs such as enabling Cgroups and setting the GPU memory size. As well for some enabling KMSG in the LXC. While all this was working fine in its current state, I wanted to move this lab to a vlan, to segment lab ip range from the proper lan(using a vlan with crosstalk, so no isolation). In Proxmox this is a few straightforward checkboxes, and a textfield, for the Raspberries, this would become harder than it needed to be. In Ubuntu 21.10 they removed the linux-modules-extra-raspi package from the base image, this meant that the kernel module 802.1q was not able to be loaded, and thus no vlan. I did a digression into HashiCorp’s Packer to determine if it would be worth it to bundle this myself, it wasn’t. While I really wanted the playbook to do it all, it now meant I had to ssh into the two nodes after the initial playbook run. I did help myself here, as I bootstrapped the Raspberries with a network-config file already set up with the wanted network configuration, all I had to do after the initial run was to alter this file on the host, setting dhcp to false on the untagged interface.\nCould I have done this in Ansible? yes. Did I want to? no, I already locked myself out too many times writing this playbook.\nKubeadm init Now that the nodes OS is provisioned, and kubelets are running, it was time to set up the kubernetes cluster. I choose to go with Kubeadm for bootstrapping the cluster, simply because it’s easier than “the hard way”, and tested in production systems.\nBefore running the command, you need to make some decisions on networking, as certain solutions need Kubeadm to make changes to its configuration.\nSince the LXC is going to be the controlplane, that’s where I initialize the cluster. Proxmox’s kernel isn’t fully compatible with the requirements Kubeadm has set, so the cluster had to be initialized by bypassing the check for the config kernel setting.\n1 [ERROR SystemVerification]: failed to parse kernel config: unable to load kernel module: \"configs\", output: \"modprobe: FATAL: Module configs not found in directory /lib/modules/5.15.30-2-pve\", err: exit status 1 The command I ended up using was\n1 kubeadm init --pod-network-cidr=10.244.0.0/16 --ignore-preflight-errors=SystemVerification In the output of the init command, I made a note of the join command. The output also tell you that now time is the time to get a CNI3, once again I went the easy route with Flannel, as it just works out of the box. For my next iteration I am considering Weave, due to it supporting network policies.\nThe output helpfully gives you a copy-paste of the commands you need to do if you want to interface with the cluster outside of the root account, the file in this example is also the file you want to have on the computer you plan to manage the cluster by(or you can already here look at creating a dedicated certificate, rather than the one Kubeadm generates).\nThe next step was running the command with the join token I noted earlier on the other nodes, this should be a painless process. It was, I was now watching the command kubectl get nodes, and looking at all the nodes getting to a ready state, it took some convincing, but after a couple of test pods also going into ready state I now had a working cluster.\nGitOps Because I also had video lectures with practice tests I was doing at the same time, I were already comfortable with kubectl and dealing with bare manifests. I had no desire to do that in this lab, I therefore dove head first into Helm4 and Argo CD5.\nThis section is going to be a bit messy, as I will be linking different commits, from a git-tree that has been rewritten( to remove my previous attempt at trying this, having no idea what I was doing). This is why the first commit has a structure, and a bunch of files. Getting started I installed the Helm binary and went to town. I created a namespace for Argo, then I installed it trough the chart, specifying the namespace. My next step was looking into how I could have Argo manage itself, I quickly discovered the “App of apps” pattern, realizing my initial deployment could have been done smoother. I then tore down the deployment and followed the Helm example in Argos docs, creating the subchart(if you for some reason are following along, you need to pull down the chart locally before you can install it). The install command is now a bit different, as I am using a local chart folder.\n1 helm install argo-cd Charts/argo-cd/ --namespace argo-cd This chart has some values in values.yaml I remember struggling with in my first attempt. I disable dex, as I have no need for external authentication to Argo at the moment. I told the chart to change the service type to nodeport, mainly because I didn’t feel like setting up the kubectl port-forward each time I want to look at it. I created another admin account with the Roxedus username (to set the password, you need to use the argo cli/api to reset it with another admin account). As I have no method to generate tls certificates, Argo gets the --insecure argument which tells it to allow unencrypted access to the page. Note the comment at the bottom, in my first attempt at this, Argo had some issues with it’s pipeline regarding building and pushing for arm64.\nThe next commit does a couple of things, but I am mainly focusing on apps subfolder for the moment. In this folder, I define the root application, thanks to Argos application custom resource. The important bit here, is apps/templates/argo-cd.yaml, it tells Argo to create a internal application, to track a subfolder in git, this is the same git repository in which the ArgoCD helm subchart lives. Once I tell Argo to watch this apps folder, I can do all creation and destruction of deployments trough git. There is a couple of ways to tell Argo on how to watch this repository, you can use kubectl to apply a manifest, using the argocd cli, or the easy way, trough the webui. There wasn’t much info needed to fill here, just the repo url, and optionally a subfolder. To keep some order, I decided to keep all application definitions in a apps folder. Once this was created, I immediately saw it pick up the Argocd chart, and it started to do a couple of remediations, as it had deviated a bit from the initial install.\nKeeping updated In the same commit as the one deploying the Argo chart, there is also a cronjob definition for Renovatebot, which is a tool for checking a git repository for new version numbers. As the Argo subchart is using a specific version tag, and Renovate speaking Helm repos, Renovate is able to see that there is a new version. It then creates a pull-request on my git repo to update this file to the new version.\nThe image Renovate publish as default has all the tooling it needs in order to parse and understand all the package systems it knows, this makes it very large, the image is 1.3 GB compressed. The alternate slim tag only contains node, and relies on the ability to spin up additional containers in order to load tooling. Because it holds all the tooling, it also makes it unwieldy to offer in multiple architectures, I needed to allow it to run on the one amd64 machine in this cluster, the controlplane. Therefore the job is set to run on nodes with the built-in architecture label, however this is not enough, as controlplanes are by default not allowed to run workloads, you need to tell the workload it is allowed to run on the controlplane, this is done with tolerations.\nThis cronjob got deployed in a “CI” application in Argo, because I figured having a dedicated namespace for CI might be beneficial in the future.\nUnlike linitng-tools and other project-specific tools, Renovate is quite flexible with the location and naming of its project-specific configuration, I choose to call it .renovaterc. This file evolved a little of the span of this journey, but it mostly stayed the same, set to watch over pure Kubernetes manifests, Helm charts and Argo-cd.\nIt is very easy to track changes, and updates I have approved(merged) or declined(closed) in Gitea.\nAdditional Infrastructure I consider many of the objects in this section as “meta-objects”, objects that needs to exist, but does not directly tie into a deployment or application.\nConnecting people Any good homelab these days will result in some webguis you may want to use in order to keep in track with the current state of the lab, this one is no different. This desire spawned many questions and failed solutions, yet trough all the failed attempts, I managed to keep Traefik as a constant in this endeavour.\nThe first and simple solution I had in mind was using Cloudflare tunnels to handle external traffic. This was working, but relied on me manually creating tunnels, and cloudflare being in charge of managing TLS, which I don’t love.\nThe next solution is based on a neat project, justmiles/traefik-cloudflare-tunnel. It does exactly the steps I previously did manually, but this does it programmatically, and by reading the Traefik routes. Being on arm64 really started hurting here, as cloudflared6 at the time did not build images for arm (Traefik-cloudflare-tunnel still doesn’t, February 2023). This lead to another tangent, as I cobbled together some build stuff using Github Actions and QEMU to build these projects for arm, all while not modifying the source. This was all done using docker-bake, in my pipelines repo.\nNone of these solutions is using the ingress mechanism in Kubernetes, so I kept on looking.\nBeing inspired by this post by TheOrangeOne, I decided to try rearchitecting the reverse-proxy solution running on my mediaserver, to be fronted by HAProxy for the SNI routing abilities, but I just couldn’t befriend using the proxy protocol in nginx.\nAfter coming to the realization that I will always be connected to my lan, thanks to Wireguard, I came to the conclusion of not needing to get to the cluster from the outside. This opened a new avenue of trial and error, mostly to get Traefik to listen to port 80 and 443. I went into this challenge knowing the prerequisites to get this going in Docker and plain Debian. To do this in Kubernetes, you need to tell the pod to attach to the host network, as well as to tell the process to run as root and setting sysctls. I used this setup for a while, but I was not happy having to deal with using host networking. As a side-note this also had me labeling nodes which had the dns set up.\nLittle loadbalancer that could The final solution depends on Metallb to do the heavy work. If your cluster is not in a cloud, this will do wonders for your load-balancer woes. It has a couple of working-modes, BGP or ARP(layer 2), in my lab it is working with ARP, as none of the listed limitations applies to my use-case. I set it up with 9( or is it 10?) IPs. Once this was setup, I was now able to create loadbalancer services in my cluster. This allowed me to revert the changes making Traefik running as root, and all the other changes I needed previously.\nSaving to disk One of my biggest gripe about kubernetes (or any distributed compute in general) is storage, for many of the applications you want to run, NFS is suitable, however a decent amount of the applications I am interested to eventually run in a cluster does not work well with NFS. There is a lot of solutions to this, some use a host path and lock the pod to that node, others still want to use NFS, I went for Longhorn.io which replicates files in mounts across hosts, allowing pods to migrate between nodes. Longhorn exposes a StorageClass which is a native kubernetes concept I can deal with.\nManaging certificates While I relied on Traefik’s way to handle and generate certificates, and had no problems with this, I was looking into more and more potential applications that works against the tls type of kubernetes secrets. This type was something I had in mind after looking into Traefik’s IngressRoute CRD against it’s kubernetes ingress integration. At this point I also told myself I was done using shortcuts like IngressRoute when theres built-in functionality.\nThe helm chart itself is pretty standard, I didn’t have to specify much, just some Cloudflare-specific dns stuff(This has to be a LeGo thing, as this also needed for Traefik), all the configuration I needed was provisioning a Cloudflare-issuer, you tell this spec which secret it should get the api token from.\nLike any sane person, I thought testing against the very application I need to revert the change was the best fit, so I went ahead and enabled ingress as well as cert-generation at the same time to Argo, while hoping this didn’t lock me out.\nI checked the status of the certificate request and order this triggered, to see if it generated a certificate, and sure enough, the order was fulfilled and Traefik used the certificate to serve the Argo subdomain.\n1 2 3 $ kubectl get certificaterequests.cert-manager.io -n argo-cd NAME APPROVED DENIED READY ISSUER REQUESTOR AGE argo-roxedus-com-cert-qzwf2 True True roxedus.com-cloudflare system:serviceaccount:cert-manager:cert-manager 7d 1 2 3 $ kubectl get orders.acme.cert-manager.io -n argo-cd NAME STATE AGE argo-roxedus-com-cert-qzwf2-3069573698 valid 7d Keeping secrets Kubernetes secrets are stored in etcd as plaintext by default, which in my lab isn’t really that big of a deal, but it is something I wanted to prevent if I could. Before starting on this adventure I have always wanted to get some hands-on with HashiCorp Vault, so it was the obvious choice when I needed a secrets manager. Deployment was a breeze, just tell the chart what type of storage class to use, and we are of to the races. I then configured Vault to use kubernetes as a authentication method with short lived tokens.\nI could have used Vault’s injector, but I choose to look for a solution that presents native kubernetes objects in the end, this way I can introduce new applications with less changes to helm charts. This is where external-secrets.io comes into play. Much like the Cert Manager chart, there was not much needed in the helm values, as most configuration happens in dedicated manifests.\nFor external-secrets to create a kubernetes secret, one would need to create a ExternalSecret object telling the operator which key and property the secret has in Vault, and to which name the kubernetes secret should have, and optionally which namespace this should target. The ExternalSecret require a SecretStore (or optionally a ClusterSecretStore) to tell the operator about the external secret manager, and how to authorize against the manager, which in this case is trough the short lived tokens.\nMy first deployment Although SearXNG is the first deployment I did outside of helm, it took shape over multiple iterations, as my portfolio of meta-objects evolved. The first version of this used only a NodePort service and a ConfigMap to accompany the deployment. It’s history is a good representation of how this whole cluster evolved, as I used this as my test application. It includes changes like using the Traefik IngresRoute CRD to migrating to Ingress.\nCome certification After finishing the courses on KodeKloud, as well as building this cluster, I was content with my skills against the outlined areas in the CKA curriculum, and went ahead to do a run in KillerShell(you get two runs in this simulator with the purchase of the exam trough Linux Foundation). It’s a good thing this simulator is supposed to be more challenging than the exam, because I was starting to question my skills, regardless, I went ahead and scheduled the exam for a friday, as this was a slow day in my calendar. When I exited the room after submitting my answers, I did not have high hopes, but was still exited to get the result within 24 hours, worst case I could schedule my included second attempt.\nWhile minding my own business, cleaning my apartment, the email came, I passed the exam! I was surprised I managed to land this without much “real world” experience.\nI started the following week being quite happy with the achievement from last week, I had set a goal to get CKA and CKAD done by March, and was half-way already in January. We were multiple people at the office going trough different certifications, since the atmosphere still was set on certifications, I figured I should at least look at the curriculum for CKAD, and they looked quite manageable. On Tuesday I was back on the course-grind, by Thursday I had started a run on KillerShell, this time for CKAD.\nOn Monday I scheduled the CKAD exam for the following day, I was quite confident, as the curriculum overlaps a great deal with the CKA, and the fact that I had unknowingly done a lot of the tasks the CKAD focuses on in my own cluster. Tuesday went, and my confidence were still present while leaving the room.\nThe wait for this email was a bit more nerveracking, mostly because I couldn’t be irresponsible by gaming/sleep trough most of the hours, like I could over the weekend. The overall wait time also ended up being a while longer, but it finally came. I passed this one too! I almost expected to pass this one, but it was very comforting receiving confirmation.\nNow work started picking up pace, so I had to shift my focus towards other stuff for a while.\nAs the summer vacation began to close in, work slowed down, I could now focus some more on certifications.\nI had one goal for the summer, which was CKS, I was mentally prepared for a month of heavy, and probably demotivating learning. For this certification I also decided to stick to KodeKlouds courses, mainly because their labs resonate with my way of learning. My expectations were quickly proved wrong, as I found most of the topics interesting, or it brought up scenarios I already have thought about, and solved (ie. SSH key-auth). This course also made me understand AppArmor, which I had brushed off years ago as very advanced.\nI spent a couple of weeks on going trough the courses and labs, before I adventured into KillerShell, where I once again was happy with my results. I scheduled and took the exam the same week as I went trough KillerShell.\nWhile I skipped a whole task in the exam, I still managed to pass this too.\nLinux Containers is an operating-system-level virtualization method for running multiple isolated Linux systems (containers) on a control host using a single Linux kernel. Wikipedia ↩︎\nInitiative created by Docker to define standards for multiple aspects regarding running containers. OpenContainers ↩︎\nCalled Network Plugin in the Kubernetes docs, a component that handles networking between pods. ↩︎\nThe package manager for Kubernetes. helm.sh ↩︎\nArgo CD is a declarative, GitOps continuous delivery tool for Kubernetes. argoproj.github.io ↩︎\nSoftware responsible for receiving the traffic from a Cloudflare tunnel. Github ↩︎\n","wordCount":"3862","inLanguage":"en","image":"https://nonsense.fyi/images/captain.jpg","datePublished":"2023-10-13T00:00:00+02:00","dateModified":"2023-10-13T00:00:00+02:00","author":{"@type":"Person","name":"Simen Røstvik"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://nonsense.fyi/posts/road-to-ckad/"},"publisher":{"@type":"Organization","name":"nonsense.fyi","logo":{"@type":"ImageObject","url":"https://nonsense.fyi/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://nonsense.fyi/ accesskey=h title="nonsense.fyi (Alt + H)">nonsense.fyi</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://nonsense.fyi/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://nonsense.fyi/tags/ title=tags><span>tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://nonsense.fyi/>Home</a>&nbsp;»&nbsp;<a href=https://nonsense.fyi/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Road to CKAD</h1><div class=post-description>My adventure in Certifying myself in Kubernetes</div><div class=post-meta><span title='2023-10-13 00:00:00 +0200 +0200'>October 13, 2023</span>&nbsp;·&nbsp;19 min&nbsp;·&nbsp;Simen Røstvik</div></header><figure class=entry-cover><img loading=eager srcset="https://nonsense.fyi/posts/road-to-ckad/images/captain_hubec8d7a291f6048741ff99462904e67e_1259597_360x0_resize_q75_box.jpg 360w ,https://nonsense.fyi/posts/road-to-ckad/images/captain_hubec8d7a291f6048741ff99462904e67e_1259597_480x0_resize_q75_box.jpg 480w ,https://nonsense.fyi/posts/road-to-ckad/images/captain_hubec8d7a291f6048741ff99462904e67e_1259597_720x0_resize_q75_box.jpg 720w ,https://nonsense.fyi/posts/road-to-ckad/images/captain_hubec8d7a291f6048741ff99462904e67e_1259597_1080x0_resize_q75_box.jpg 1080w ,https://nonsense.fyi/posts/road-to-ckad/images/captain_hubec8d7a291f6048741ff99462904e67e_1259597_1500x0_resize_q75_box.jpg 1500w ,https://nonsense.fyi/posts/road-to-ckad/images/captain.jpg 5666w" sizes="(min-width: 768px) 720px, 100vw" src=https://nonsense.fyi/posts/road-to-ckad/images/captain.jpg alt="A captain overseeing his ship" width=5666 height=3541></figure><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#so-i-took-some-certs aria-label="So I took some certs">So I took some certs</a></li><li><a href=#preparation aria-label=Preparation>Preparation</a><ul><li><a href=#proxmox-and-lxc aria-label="Proxmox and LXC">Proxmox and LXC</a><ul><li><a href=#controlplane-lxc aria-label="Controlplane LXC">Controlplane LXC</a></li></ul></li><li><a href=#playing-with-the-playbook aria-label="Playing with the playbook">Playing with the playbook</a></li></ul></li><li><a href=#kubeadm-init aria-label="Kubeadm init">Kubeadm init</a></li><li><a href=#gitops aria-label=GitOps>GitOps</a><ul><li><a href=#getting-started aria-label="Getting started">Getting started</a></li><li><a href=#keeping-updated aria-label="Keeping updated">Keeping updated</a></li></ul></li><li><a href=#additional-infrastructure aria-label="Additional Infrastructure">Additional Infrastructure</a><ul><li><a href=#connecting-people aria-label="Connecting people">Connecting people</a><ul><li><a href=#little-loadbalancer-that-could aria-label="Little loadbalancer that could">Little loadbalancer that could</a></li></ul></li><li><a href=#saving-to-disk aria-label="Saving to disk">Saving to disk</a></li><li><a href=#managing-certificates aria-label="Managing certificates">Managing certificates</a></li><li><a href=#keeping-secrets aria-label="Keeping secrets">Keeping secrets</a></li></ul></li><li><a href=#my-first-deployment aria-label="My first deployment">My first deployment</a></li><li><a href=#come-certification aria-label="Come certification">Come certification</a></li></ul></div></details></div><div class=post-content><h2 id=so-i-took-some-certs>So I took some certs<a hidden class=anchor aria-hidden=true href=#so-i-took-some-certs>#</a></h2><p>Ive recently gone trough the Kubernetes Administrator, Developer and Security Specialist certifications. In typical fashion I broke some stuff on my way to get there.</p><div class="admonition note"><div class=content>The git references in this article are not up-to-date, as later deployments has made me consolidating multiple repos to a single infra repo.</div></div><h2 id=preparation>Preparation<a hidden class=anchor aria-hidden=true href=#preparation>#</a></h2><p>This is everything I used to prepare for these certifications:</p><ul><li>Time</li><li>Some smol computing boys, three Raspberry PI 4 8GB was hurt for this blogpost</li><li>Lectures with practical tests, I went with the courses from KodeKloud</li><li>A not so big LXC running on Proxmox 7</li><li>Patience</li><li>Time</li></ul><h3 id=proxmox-and-lxc>Proxmox and LXC<a hidden class=anchor aria-hidden=true href=#proxmox-and-lxc>#</a></h3><p>For some unrelated infrastructure, I set up Proxmox. I attempted to put most of the configuration in Ansible, but its just not feasible in the long run. The Proxmox host is however not fully ClickOps based, as packages, swap and <a href=https://git.roxedus.dev/Roxedus/Infra/src/commit/946284e0bed7a334949755be45cfcb9b55a7e881/ansible/roles/proxmox/templates/get_cert.sh.j2 target=_blank rel=noopener>the certificate(fetched from OpnSense)</a> is managed by the playbook.</p><h4 id=controlplane-lxc>Controlplane LXC<a hidden class=anchor aria-hidden=true href=#controlplane-lxc>#</a></h4><p>Because sourcing additional Raspberry PI&rsquo;s was an impossible task (and still isn&rsquo;t easy), I thought &ldquo;Hey, my Proxmox host is only doing Home-Assistant right know, I can squeeze more onto it&rdquo;.</p><p>Since I am who I am, I decided to dabble in something new, enter Linux Containers(lxc)<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>.</p><p>The reason for researching this method is straightforward; VMs are heavy, OCI<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> containers are light. I needed something in between.
This host is limited on resources, but not starved for them. Therefore avoiding running another kernel and subsystem would be preferred, this ruled out Virtual Machines.
Another option I am very comfortable with, is Docker. However, (stock) Kubernetes really wants a fully-fledged init system running and I am not crazy enough to run a complete Systemd instance in a Docker container.</p><p>As it took multiple attempts to get the LXC configured to make kubeadm happy, I saved that file once all the issues was ironed out. It looks something like this:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-yml data-lang=yml><span class=line><span class=cl><span class=nt>arch</span><span class=p>:</span><span class=w> </span><span class=l>amd64</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>cores</span><span class=p>:</span><span class=w> </span><span class=m>2</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>hostname</span><span class=p>:</span><span class=w> </span><span class=l>controlplane</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>memory</span><span class=p>:</span><span class=w> </span><span class=m>4096</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>net0</span><span class=p>:</span><span class=w> </span><span class=l>name=eth0,bridge=vmbr0,firewall=1,gw=&lt;gateway ip&gt;,hwaddr=&lt;mac&gt;,ip=&lt;ip with cidir range&gt;,type=veth</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>ostype</span><span class=p>:</span><span class=w> </span><span class=l>ubuntu</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>rootfs</span><span class=p>:</span><span class=w> </span><span class=l>&lt;DiskInfo&gt;,size=50G</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>searchdomain</span><span class=p>:</span><span class=w> </span><span class=l>kube.&lt;domain&gt;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>swap</span><span class=p>:</span><span class=w> </span><span class=m>0</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>features</span><span class=p>:</span><span class=w> </span><span class=l>fuse=1,mount=nfs,nesting=1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>lxc.apparmor.profile</span><span class=p>:</span><span class=w> </span><span class=l>unconfined</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>lxc.cap.drop</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>lxc.cgroup.devices.allow</span><span class=p>:</span><span class=w> </span><span class=l>a</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>lxc.mount.auto</span><span class=p>:</span><span class=w> </span><span class=l>proc:rw sys:rw</span><span class=w>
</span></span></span></code></pre></td></tr></table></div></div><p>I later tried to define this LXC with Terraform, but ran mostly into auth issues. This LXC needed to be privileged, which means it cannot be created by anyone else then the &ldquo;root&rdquo; user, making a service account useless. I have also opted for using MFA on the root account, which made user/password moot.</p><h3 id=playing-with-the-playbook>Playing with the playbook<a hidden class=anchor aria-hidden=true href=#playing-with-the-playbook>#</a></h3><p>As mentioned, I have a Ansible playbook for infrastructure running inside my lan. This playbook gives me a familiar and uniform environment for all hosts.
It handles <a href=https://git.roxedus.dev/Roxedus/Infra/src/commit/946284e0bed7a334949755be45cfcb9b55a7e881/ansible/group_vars/all.yml#L21-L37 target=_blank rel=noopener>common packages</a> I have come to expect being present on the machine im working on, while setting up <a href=https://git.roxedus.dev/Roxedus/Infra/src/commit/946284e0bed7a334949755be45cfcb9b55a7e881/ansible/tasks/users.yml target=_blank rel=noopener>my user with a public key</a>, and the <a href=https://git.roxedus.dev/Roxedus/Infra/src/commit/946284e0bed7a334949755be45cfcb9b55a7e881/ansible/tasks/omp.yml target=_blank rel=noopener>prompt in my shell</a>.</p><p>To get my environment ready, I wrote a Kubernetes role into my existing <a href=https://git.roxedus.dev/Roxedus/Infra/src/commit/946284e0bed7a334949755be45cfcb9b55a7e881/ansible/roles/kubernetes target=_blank rel=noopener>Ansible Infra playbook</a>, this takes care of both <a href=https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#before-you-begin target=_blank rel=noopener>Kubeadm&rsquo;s</a> and Kubernetes&rsquo; requirements.
This includes <a href=https://git.roxedus.dev/Roxedus/Infra/src/commit/946284e0bed7a334949755be45cfcb9b55a7e881/ansible/roles/kubernetes/tasks/node.yml#L1-L14 target=_blank rel=noopener>disabling swap</a>, setting up <a href=https://git.roxedus.dev/Roxedus/Infra/src/commit/946284e0bed7a334949755be45cfcb9b55a7e881/ansible/run.yml#L134-L138 target=_blank rel=noopener>containerd as a runtime</a>(thanks <a href=https://github.com/geerlingguy/ansible-role-containerd target=_blank rel=noopener>Jeff</a>) and a whole lot of <a href=https://git.roxedus.dev/Roxedus/Infra/src/commit/946284e0bed7a334949755be45cfcb9b55a7e881/ansible/roles/kubernetes/tasks/node.yml#L80-L115 target=_blank rel=noopener>kernel tuning</a>. Using some roles in the Ansible Galaxy might have saved me some time here, but before I landed on using Jeff&rsquo;s containerd role, I tried to use some of the cri-o roles, but I spent too much time digging in apt repos to mitigate the fact that Kubernetes with ARM64 nodes is less adopted than I thought(more on this later). I quickly abandoned the idea of using a Galaxy role for cri-o, after struggling to get cri-o going at all, I gave up on cri-o altogether and settled for the containerd role.</p><p>Since my lab is run on somewhat unconventional setups, I had to make several changes for <a href=https://git.roxedus.dev/Roxedus/Infra/src/commit/946284e0bed7a334949755be45cfcb9b55a7e881/ansible/roles/kubernetes/tasks/node.yml#L42-L63 target=_blank rel=noopener>Raspberry PIs</a> such as enabling Cgroups and setting the GPU memory size. As well for some <a href=https://git.roxedus.dev/Roxedus/Infra/src/commit/946284e0bed7a334949755be45cfcb9b55a7e881/ansible/roles/kubernetes/tasks/node.yml#L27-L38 target=_blank rel=noopener>enabling KMSG in the LXC</a>.
While all this was working fine in its current state, I wanted to move this lab to a vlan, to segment lab ip range from the proper lan(using a vlan with crosstalk, so no isolation). In Proxmox this is a few straightforward checkboxes, and a textfield, for the Raspberries, this would become harder than it needed to be. In Ubuntu 21.10 they removed the <code>linux-modules-extra-raspi</code> package from the base image, this meant that the kernel module 802.1q was not able to be loaded, and thus no vlan. I did a digression into HashiCorp&rsquo;s Packer to determine if it would be worth it to bundle this myself, it wasn&rsquo;t. While I really wanted the playbook to do it all, it now meant I had to ssh into the two nodes after the initial playbook run. I did help myself here, as I bootstrapped the Raspberries with a <a href=https://git.roxedus.dev/Roxedus/Infra/src/commit/946284e0bed7a334949755be45cfcb9b55a7e881/cloud-init/arm-ubuntu/network-config target=_blank rel=noopener><code>network-config</code> file</a> already set up with the wanted network configuration, all I had to do after the initial run was to alter this file on the host, setting dhcp to false on the untagged interface.</p><p>Could I have done this in Ansible? yes. Did I want to? no, I already locked myself out too many times writing this playbook.</p><hr><h2 id=kubeadm-init>Kubeadm init<a hidden class=anchor aria-hidden=true href=#kubeadm-init>#</a></h2><p>Now that the nodes OS is provisioned, and kubelets are running, it was time to set up the kubernetes cluster. I choose to go with Kubeadm for bootstrapping the cluster, simply because it&rsquo;s easier than <a href=https://github.com/kelseyhightower/kubernetes-the-hard-way target=_blank rel=noopener>&ldquo;the hard way&rdquo;</a>, and tested in production systems.</p><p>Before running the command, you need to make some decisions on networking, as certain solutions need Kubeadm to make changes to its configuration.</p><p>Since the LXC is going to be the controlplane, that&rsquo;s where I initialize the cluster. Proxmox&rsquo;s kernel isn&rsquo;t fully compatible with the requirements Kubeadm has set, so the cluster had to be initialized by bypassing the check for the <code>config</code> kernel setting.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>[ERROR SystemVerification]: failed to parse kernel config: unable to load kernel module: &#34;configs&#34;, output: &#34;modprobe: FATAL: Module configs not found in directory /lib/modules/5.15.30-2-pve&#34;, err: exit status 1
</span></span></code></pre></td></tr></table></div></div><p>The command I ended up using was</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>kubeadm init --pod-network-cidr<span class=o>=</span>10.244.0.0/16 --ignore-preflight-errors<span class=o>=</span>SystemVerification
</span></span></code></pre></td></tr></table></div></div><p>In the output of the init command, I made a note of the join command. The output also tell you that now time is the time to get a CNI<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>, once again I went the easy route with <a href=https://github.com/flannel-io/flannel target=_blank rel=noopener>Flannel</a>, as it just works out of the box. For my next iteration I am considering <a href=https://github.com/weaveworks/weave target=_blank rel=noopener>Weave</a>, due to it supporting network policies.</p><p>The output helpfully gives you a copy-paste of the commands you need to do if you want to interface with the cluster outside of the root account, the file in this example is also the file you want to have on the computer you plan to manage the cluster by(or you can already here look at creating a dedicated certificate, rather than the one Kubeadm generates).</p><p>The next step was running the command with the join token I noted earlier on the other nodes, this should be a painless process. It was, I was now watching the command <code>kubectl get nodes</code>, and looking at all the nodes getting to a ready state, it took some convincing, but after a couple of test pods also going into ready state I now had a working cluster.</p><h2 id=gitops>GitOps<a hidden class=anchor aria-hidden=true href=#gitops>#</a></h2><p>Because I also had video lectures with practice tests I was doing at the same time, I were already comfortable with kubectl and dealing with bare manifests. I had no desire to do that in this lab, I therefore dove head first into Helm<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup> and Argo CD<sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup>.</p><div class="admonition tip"><div class=content>This section is going to be a bit messy, as I will be linking different commits, from a git-tree that has been rewritten( to remove my previous attempt at trying this, having no idea what I was doing). This is why the first commit has a structure, and a bunch of files.</div></div><h3 id=getting-started>Getting started<a hidden class=anchor aria-hidden=true href=#getting-started>#</a></h3><p>I installed the Helm binary and went to town. I created a namespace for Argo, then I installed it trough the chart, specifying the namespace. My next step was looking into how I could have Argo manage itself, I quickly discovered the <a href=https://argo-cd.readthedocs.io/en/stable/operator-manual/cluster-bootstrapping/ target=_blank rel=noopener>&ldquo;App of apps&rdquo; pattern</a>, realizing my initial deployment could have been done smoother. I then tore down the deployment and followed the Helm example in Argos docs, <a href=https://git.roxedus.dev/Roxedus/Argo/src/commit/91cd43c740385e62c9318a19010fb699520d7ac3/Charts/argo-cd target=_blank rel=noopener>creating the subchart</a>(if you for some reason are following along, you need to pull down the chart locally before you can install it). The install command is now a bit different, as I am using a local chart folder.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>helm install argo-cd Charts/argo-cd/ --namespace argo-cd
</span></span></code></pre></td></tr></table></div></div><p>This chart has some values in <code>values.yaml</code> I remember struggling with in my first attempt. I disable dex, as I have no need for external authentication to Argo at the moment. I told the chart to change the service type to nodeport, mainly because I didn&rsquo;t feel like setting up the kubectl port-forward each time I want to look at it. I created another admin account with the Roxedus username (to set the password, you need to use the argo cli/api to reset it with another admin account). As I have no method to generate tls certificates, Argo gets the <code>--insecure</code> argument which tells it to allow unencrypted access to the page. Note the comment at the bottom, in my first attempt at this, Argo had some issues with it&rsquo;s pipeline regarding <a href=https://github.com/argoproj/argo-cd/issues/8394 target=_blank rel=noopener>building and pushing for arm64</a>.</p><p>The <a href=https://git.roxedus.dev/Roxedus/Argo/commit/1cbd509a7669370da22261281eeb645eefe97bad target=_blank rel=noopener>next commit</a> does a couple of things, but I am mainly focusing on <a href=https://git.roxedus.dev/Roxedus/Argo/src/commit/1cbd509a7669370da22261281eeb645eefe97bad/apps target=_blank rel=noopener>apps subfolder</a> for the moment. In this folder, I define the <code>root</code> application, thanks to Argos application custom resource. The important bit here, is <code>apps/templates/argo-cd.yaml</code>, it tells Argo to create a internal application, to track a subfolder in git, this is the same git repository in which the ArgoCD helm subchart lives. Once I tell Argo to watch this apps folder, I can do all creation and destruction of deployments trough git. There is a couple of ways to tell Argo on how to watch this repository, you can use <code>kubectl</code> to apply a manifest, using the <code>argocd</code> cli, or the easy way, trough the webui. There wasn&rsquo;t much info needed to fill here, just the repo url, and optionally a subfolder. To keep some order, I decided to keep all application definitions in a <code>apps</code> folder. Once this was created, I immediately saw it pick up the Argocd chart, and it started to do a couple of remediations, as it had deviated a bit from the initial install.</p><h3 id=keeping-updated>Keeping updated<a hidden class=anchor aria-hidden=true href=#keeping-updated>#</a></h3><p>In the same commit as the one deploying the Argo chart, there is also a cronjob definition for <a href=https://renovatebot.com/ target=_blank rel=noopener>Renovatebot</a>, which is a tool for checking a git repository for new version numbers. As the Argo subchart is using a specific version tag, and Renovate speaking Helm repos, Renovate is able to see that there is a new version. It then creates a pull-request on my git repo to update this file to the new version.</p><p>The image Renovate publish as default has all the tooling it needs in order to parse and understand all the package systems it knows, this makes it very large, the image is 1.3 GB <em>compressed</em>. The alternate <code>slim</code> tag only contains node, and relies on the ability to spin up additional containers in order to load tooling.
Because it holds all the tooling, it also makes it unwieldy to offer in multiple architectures, I needed to allow it to run on the one amd64 machine in this cluster, the controlplane. Therefore the job is set to run on nodes with the built-in architecture label, however this is not enough, as controlplanes are by default not allowed to run workloads, you need to tell the workload it is allowed to run on the controlplane, this is done with tolerations.</p><p>This cronjob got <a href=https://git.roxedus.dev/Roxedus/Argo/src/commit/d17fb849fa7071bfec272624c07c7d722fba6a99/apps/templates/ci.yaml target=_blank rel=noopener>deployed</a> in a &ldquo;CI&rdquo; application in Argo, because I figured having a dedicated namespace for CI might be beneficial in the future.</p><p>Unlike linitng-tools and other project-specific tools, Renovate is <a href=https://docs.renovatebot.com/configuration-options/#configuration-options target=_blank rel=noopener>quite flexible</a> with the location and naming of its project-specific configuration, I choose to call it <code>.renovaterc</code>. This file evolved a little of the span of this journey, but it mostly stayed the same, set to watch over pure Kubernetes manifests, Helm charts and Argo-cd.</p><p>It is very easy to track changes, and updates I have approved(merged) or declined(closed) in <a href="https://git.roxedus.dev/Roxedus/Argo/pulls?q=&amp;type=all&amp;sort=&amp;state=closed&amp;labels=&amp;milestone=0&amp;assignee=0&amp;poster=3" target=_blank rel=noopener>Gitea</a>.</p><h2 id=additional-infrastructure>Additional Infrastructure<a hidden class=anchor aria-hidden=true href=#additional-infrastructure>#</a></h2><p>I consider many of the objects in this section as &ldquo;meta-objects&rdquo;, objects that needs to exist, but does not directly tie into a deployment or application.</p><h3 id=connecting-people>Connecting people<a hidden class=anchor aria-hidden=true href=#connecting-people>#</a></h3><p>Any good homelab these days will result in some webguis you may want to use in order to keep in track with the current state of the lab, this one is no different. This desire spawned many questions and failed solutions, yet trough all the failed attempts, I managed to keep Traefik as a constant in this endeavour.</p><p>The first and simple solution I had in mind was using Cloudflare tunnels to handle external traffic. This was working, but relied on me manually creating tunnels, and cloudflare being in charge of managing TLS, which I don&rsquo;t love.</p><p>The next solution is based on a neat project, <a href=https://github.com/justmiles/traefik-cloudflare-tunnel/tree/master target=_blank rel=noopener>justmiles/traefik-cloudflare-tunnel</a>. It does exactly the steps I previously did manually, but this does it programmatically, and by reading the Traefik routes. Being on arm64 really started hurting here, as cloudflared<sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup> at the time did not build images for arm (Traefik-cloudflare-tunnel still doesn&rsquo;t, February 2023). This lead to another tangent, as I cobbled together some build stuff using Github Actions and QEMU to build these projects for arm, all while not modifying the source. This was all done using docker-bake, in my <a href=https://github.com/Roxedus/pipelines target=_blank rel=noopener>pipelines repo</a>.</p><p><strong>None of these solutions is using the ingress mechanism in Kubernetes, so I kept on looking.</strong></p><p>Being inspired by <a href=https://theorangeone.net/posts/exposing-your-homelab/ target=_blank rel=noopener>this post</a> by TheOrangeOne, I decided to try rearchitecting the reverse-proxy solution running on my mediaserver, to be fronted by HAProxy for the SNI routing abilities, but I just couldn&rsquo;t befriend using the proxy protocol in nginx.</p><p>After coming to the realization that I will always be connected to my lan, thanks to Wireguard, I came to the conclusion of not needing to get to the cluster from the outside. This opened a new avenue of trial and error, mostly to get Traefik to listen to port 80 and 443. I went into this challenge knowing the prerequisites to get this going in Docker and plain Debian. To do this in Kubernetes, you need to tell the pod to attach to the host network, as well as to tell the process to run as root and setting sysctls. I used this setup for a while, but I was not happy having to deal with using host networking. As a side-note this also had me labeling nodes which had the dns set up.</p><h4 id=little-loadbalancer-that-could>Little loadbalancer that could<a hidden class=anchor aria-hidden=true href=#little-loadbalancer-that-could>#</a></h4><p>The final solution depends on <a href=https://metallb.universe.tf/ target=_blank rel=noopener>Metallb</a> to do the heavy work. If your cluster is not in a cloud, this will do wonders for your load-balancer woes. It has a couple of working-modes, BGP or ARP(layer 2), in my lab it is working with ARP, as none of the <a href=https://metallb.universe.tf/concepts/layer2/#limitations target=_blank rel=noopener>listed limitations</a> applies to my use-case. I set it up with <a href=https://git.roxedus.dev/Roxedus/Argo/src/commit/58554ecda85d965d54f86e105d8990072fdc6d3c/MetaObjects/metallb-pool.yml target=_blank rel=noopener>9( or is it 10?) IPs</a>. Once this was setup, I was now able to create loadbalancer services in my cluster. This allowed me to revert the changes making Traefik running as root, and all the <a href=https://git.roxedus.dev/Roxedus/Argo/commit/fc69303300f8c94614c183d488ea057dfe6a947a target=_blank rel=noopener>other changes</a> I needed previously.</p><h3 id=saving-to-disk>Saving to disk<a hidden class=anchor aria-hidden=true href=#saving-to-disk>#</a></h3><p>One of my biggest gripe about kubernetes (or any distributed compute in general) is storage, for many of the applications you want to run, NFS is suitable, however a decent amount of the applications I am interested to eventually run in a cluster does not work well with NFS. There is a lot of solutions to this, some use a host path and lock the pod to that node, others still want to use NFS, I went for <a href=https://longhorn.io/ target=_blank rel=noopener>Longhorn.io</a> which replicates files in mounts across hosts, allowing pods to migrate between nodes. Longhorn exposes a StorageClass which is a native kubernetes concept I can deal with.</p><h3 id=managing-certificates>Managing certificates<a hidden class=anchor aria-hidden=true href=#managing-certificates>#</a></h3><p>While I relied on Traefik&rsquo;s way to handle and generate certificates, and had no problems with this, I was looking into more and more potential applications that works against the tls type of kubernetes secrets. This type was something I had in mind after looking into Traefik&rsquo;s IngressRoute CRD against it&rsquo;s kubernetes ingress integration. At this point I also told myself I was done using shortcuts like IngressRoute when theres built-in functionality.</p><p>The helm chart itself is pretty standard, I didn&rsquo;t have to specify much, just some <a href=https://git.roxedus.dev/Roxedus/Argo/src/commit/58554ecda85d965d54f86e105d8990072fdc6d3c/apps/templates/cert-manager.yaml#L23-L24 target=_blank rel=noopener>Cloudflare-specific dns stuff</a>(This has to be a LeGo thing, as this also needed for Traefik), all the configuration I needed was provisioning a <a href=https://cert-manager.io/docs/configuration/acme/dns01/cloudflare/ target=_blank rel=noopener>Cloudflare-issuer</a>, you tell this spec <a href=https://git.roxedus.dev/Roxedus/Argo/src/branch/main/MetaObjects/cert-manager-issuer.yml target=_blank rel=noopener>which secret</a> it should get the api token from.</p><p>Like any sane person, I thought testing against the very application I need to revert the change was the best fit, so I went ahead and enabled ingress as well as cert-generation at the <a href=https://git.roxedus.dev/Roxedus/Argo/commit/610e01fe3d569ffe173256fb3471cbdb40b3026d target=_blank rel=noopener>same time</a> to Argo, while hoping this didn&rsquo;t lock me out.</p><p>I checked the status of the certificate request and order this triggered, to see if it generated a certificate, and sure enough, the order was fulfilled and Traefik used the certificate to serve the Argo subdomain.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>$ kubectl get certificaterequests.cert-manager.io -n argo-cd
</span></span><span class=line><span class=cl>NAME                          APPROVED   DENIED   READY   ISSUER                   REQUESTOR                                         AGE
</span></span><span class=line><span class=cl>argo-roxedus-com-cert-qzwf2   True                True    roxedus.com-cloudflare   system:serviceaccount:cert-manager:cert-manager   7d
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>$ kubectl get orders.acme.cert-manager.io -n argo-cd
</span></span><span class=line><span class=cl>NAME                                     STATE   AGE
</span></span><span class=line><span class=cl>argo-roxedus-com-cert-qzwf2-3069573698   valid   7d
</span></span></code></pre></td></tr></table></div></div><h3 id=keeping-secrets>Keeping secrets<a hidden class=anchor aria-hidden=true href=#keeping-secrets>#</a></h3><p>Kubernetes secrets are stored in etcd <a href=https://kubernetes.io/docs/concepts/configuration/secret/ target=_blank rel=noopener>as plaintext by default</a>, which in my lab isn&rsquo;t really that big of a deal, but it is something I wanted to prevent if I could. Before starting on this adventure I have always wanted to get some hands-on with HashiCorp Vault, so it was the obvious choice when I needed a secrets manager. Deployment was a breeze, just tell the chart what type of storage class to use, and we are of to the races. I then configured Vault to use kubernetes as a authentication method with <a href=https://developer.hashicorp.com/vault/docs/auth/kubernetes#how-to-work-with-short-lived-kubernetes-tokens target=_blank rel=noopener>short lived tokens</a>.</p><p>I could have used Vault&rsquo;s <a href=https://developer.hashicorp.com/vault/docs/platform/k8s/injector target=_blank rel=noopener>injector</a>, but I choose to look for a solution that presents native kubernetes objects in the end, this way I can introduce new applications with less changes to helm charts. This is where <a href=https://external-secrets.io/ target=_blank rel=noopener>external-secrets.io</a> comes into play. Much like the Cert Manager chart, there was not much needed in the helm values, as most configuration happens in dedicated manifests.</p><p>For external-secrets to create a kubernetes secret, one would need to create a <a href=https://git.roxedus.dev/Roxedus/Argo/src/commit/7809f69c914e26a8b5efee600ff5f585a8962496/MetaObjects/cloudflare-keys.yml target=_blank rel=noopener>ExternalSecret</a> object telling the operator which key and property the secret has in Vault, and to which name the kubernetes secret should have, and optionally which namespace this should target. The ExternalSecret require a SecretStore (or optionally a <a href=https://git.roxedus.dev/Roxedus/Argo/src/commit/7809f69c914e26a8b5efee600ff5f585a8962496/MetaObjects/secret-store.yml target=_blank rel=noopener>ClusterSecretStore</a>) to tell the operator about the external secret manager, and how to authorize against the manager, which in this case is trough the short lived tokens.</p><h2 id=my-first-deployment>My first deployment<a hidden class=anchor aria-hidden=true href=#my-first-deployment>#</a></h2><p>Although SearXNG is the first deployment I did outside of helm, it took shape over multiple iterations, as my portfolio of meta-objects evolved. The first version of this used only a NodePort service and a ConfigMap to accompany the deployment. It&rsquo;s <a href=https://git.roxedus.dev/Roxedus/Argo/commits/commit/076cbdf6a2a0f659ca7ecb169b0cc3f1ec15c75f/Deployments/searxng.yaml target=_blank rel=noopener>history</a> is a good representation of how this whole cluster evolved, as I used this as my test application. It includes changes like using the Traefik <a href=https://git.roxedus.dev/Roxedus/Argo/commit/e6eb73947578ccfae672963782f61a7be862af64 target=_blank rel=noopener>IngresRoute CRD</a> to migrating to <a href=https://git.roxedus.dev/Roxedus/Argo/commit/fa445c91f64fcd604ec79e22c34d856fa30563f0 target=_blank rel=noopener>Ingress</a>.</p><hr><h2 id=come-certification>Come certification<a hidden class=anchor aria-hidden=true href=#come-certification>#</a></h2><p>After finishing the courses on KodeKloud, as well as building this cluster, I was content with my skills against the outlined areas in the <a href=https://github.com/cncf/curriculum/blob/master/old-versions/CKA_Curriculum_v1.26.pdf target=_blank rel=noopener>CKA</a> curriculum, and went ahead to do a run in <a href=https://killer.sh/ target=_blank rel=noopener>KillerShell</a>(you get two runs in this simulator with the purchase of the exam trough Linux Foundation). It&rsquo;s a good thing this simulator is supposed to be more challenging than the exam, because I was starting to question my skills, regardless, I went ahead and scheduled the exam for a friday, as this was a slow day in my calendar. When I exited the room after submitting my answers, I did not have high hopes, but was still exited to get the result within 24 hours, worst case I could schedule my included second attempt.</p><p>While minding my own business, cleaning my apartment, the email came, I passed the exam! I was surprised I managed to land this without much &ldquo;real world&rdquo; experience.</p><p>I started the following week being quite happy with the achievement from last week, I had set a goal to get CKA and CKAD done by March, and was half-way already in January. We were multiple people at the office going trough different certifications, since the atmosphere still was set on certifications, I figured I should at least look at the curriculum for CKAD, and they looked quite manageable. On Tuesday I was back on the course-grind, by Thursday I had started a run on KillerShell, this time for CKAD.</p><p>On Monday I scheduled the CKAD exam for the following day, I was quite confident, as the <a href=https://github.com/cncf/curriculum/blob/master/old-versions/CKAD_Curriculum_v1.26.pdf target=_blank rel=noopener>curriculum</a> overlaps a great deal with the CKA, and the fact that I had unknowingly done a lot of the tasks the CKAD focuses on in my own cluster. Tuesday went, and my confidence were still present while leaving the room.</p><p>The wait for this email was a bit more nerveracking, mostly because I couldn&rsquo;t be irresponsible by gaming/sleep trough most of the hours, like I could over the weekend. The overall wait time also ended up being a while longer, but it finally came. I passed this one too! I almost expected to pass this one, but it was very comforting receiving confirmation.</p><p>Now work started picking up pace, so I had to shift my focus towards other stuff for a while.</p><p>As the summer vacation began to close in, work slowed down, I could now focus some more on certifications.</p><p>I had one goal for the summer, which was CKS, I was mentally prepared for a month of heavy, and probably demotivating learning. For this certification I also decided to stick to KodeKlouds courses, mainly because their labs resonate with my way of learning. My expectations were quickly proved wrong, as I found most of the topics interesting, or it brought up scenarios I already have thought about, and solved (ie. SSH key-auth). This course also made me understand AppArmor, which I had brushed off years ago as very advanced.</p><p>I spent a couple of weeks on going trough the courses and labs, before I adventured into KillerShell, where I once again was happy with my results. I scheduled and took the exam the same week as I went trough KillerShell.</p><p>While I skipped a whole task in the exam, I still managed to pass this too.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Linux Containers is an operating-system-level virtualization method for running multiple isolated Linux systems (containers) on a control host using a single Linux kernel. <a href=https://en.wikipedia.org/wiki/LXC target=_blank rel=noopener>Wikipedia</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>Initiative created by Docker to define standards for multiple aspects regarding running containers. <a href=https://opencontainers.org/ target=_blank rel=noopener>OpenContainers</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>Called Network Plugin in the <a href=https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/ target=_blank rel=noopener>Kubernetes docs</a>, a component that handles networking between pods.&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>The package manager for Kubernetes. <a href=https://helm.sh/ target=_blank rel=noopener>helm.sh</a>&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p>Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes. <a href=https://argoproj.github.io/ target=_blank rel=noopener>argoproj.github.io</a>&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6><p>Software responsible for receiving the traffic from a Cloudflare tunnel. <a href=https://github.com/cloudflare/cloudflared target=_blank rel=noopener>Github</a>&#160;<a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://nonsense.fyi/tags/kubernetes/>Kubernetes</a></li><li><a href=https://nonsense.fyi/tags/homelab/>Homelab</a></li><li><a href=https://nonsense.fyi/tags/certification/>Certification</a></li></ul><nav class=paginav><a class=next href=https://nonsense.fyi/posts/templating-templates-with-templates/><span class=title>Next »</span><br><span>Templating templates with templates</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Road to CKAD on x" href="https://x.com/intent/tweet/?text=Road%20to%20CKAD&amp;url=https%3a%2f%2fnonsense.fyi%2fposts%2froad-to-ckad%2f&amp;hashtags=Kubernetes%2cHomelab%2cCertification"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Road to CKAD on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fnonsense.fyi%2fposts%2froad-to-ckad%2f&amp;title=Road%20to%20CKAD&amp;summary=Road%20to%20CKAD&amp;source=https%3a%2f%2fnonsense.fyi%2fposts%2froad-to-ckad%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Road to CKAD on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fnonsense.fyi%2fposts%2froad-to-ckad%2f&title=Road%20to%20CKAD"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Road to CKAD on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fnonsense.fyi%2fposts%2froad-to-ckad%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Road to CKAD on whatsapp" href="https://api.whatsapp.com/send?text=Road%20to%20CKAD%20-%20https%3a%2f%2fnonsense.fyi%2fposts%2froad-to-ckad%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Road to CKAD on telegram" href="https://telegram.me/share/url?text=Road%20to%20CKAD&amp;url=https%3a%2f%2fnonsense.fyi%2fposts%2froad-to-ckad%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Road to CKAD on ycombinator" href="https://news.ycombinator.com/submitlink?t=Road%20to%20CKAD&u=https%3a%2f%2fnonsense.fyi%2fposts%2froad-to-ckad%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer><script src=https://giscus.app/client.js data-repo=Roxedus/nonsense.fyi data-repo-id="MDEwOlJlcG9zaXRvcnkyNTkxMzY1NDE=" data-category=Announcements data-category-id=DIC_kwDOD3IcHc4B-8g4 data-mapping=pathname data-reactions-enabled=1 data-emit-metadata=0 data-theme=preferred_color_scheme crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2024 <a href=https://nonsense.fyi/>nonsense.fyi</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><div id=current-visitors-container></div><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>